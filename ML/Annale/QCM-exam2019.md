# Exercice 1
1. Oui. Eh bien pourquoi pas :joy:
2. Faux, si y'a un mauvais score en apprentissage c'est pas du surapprentissage :sob:
3. Oui on leur donne plus de poids $K\sum \xi$ plus grand dans plus grande pénalisation
4. Mauvaise généralisation donc overfitting
5. Faux, on utilise Bellman pour les MDP, backprop pour NN
6. Vrais
7. Vrais, faire le calcul du gradient de cette loss (voir fiche)
8. Ni l'un ni l'autre, on peut avoir les deux en mettant un moins 
9. Faux, faire le dessin et regarder la tête du coef directeur de la dériver, il est d'abord negatif puis positif
10. Faux, c'est le bagging ça
11. Vrais
12. ~Vrais. Arg je sais jamais si il faut qu'il soit naif ou pas sdnfd
13. 