# Exercice 1 
1. Vrais, propriété du cours sur les noyaux, démontré en TD
2. Faux, la régression logistique se base sur une frontière linéaire, transformé par la sigmoide
3. ~Faux, je me vois mal écrire ça sous la forme d'une produit scalaire
4. [Faux](https://stackoverflow.com/questions/41682781/why-does-naive-bayes-fail-to-solve-xor) 
5. Faux, overfitting
6. Faux
7. Faux, ce n'est pas une mesure directe du bruit dans les données
8. Vrais, VC-dim forte == plus d'overfit potentiel
9. ~Vrais
10. Vrais, SVM = modèle plus complexe = capable de fit plus les données complexe = VC-dim plus forte qu'un simple perceptron (chatgpt approved)
11. Faux, c'est une bonne indication mais uniquement indep => cov=0
12. Vrais, calcul : $P(\mu)P(X|\mu) / P(X)$ multiplication de deux gaussiennes + div par une constante
13. ? 
14. ? Vrais
15. ? Mais de quoi il parle mdr
16. ~ j'hésite fort
