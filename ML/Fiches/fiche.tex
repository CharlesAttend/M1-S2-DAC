\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=1.27cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein's Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Fiche ML}
\author{Charles Vin}
\date{S2-2023}

\begin{document}
\maketitle
\section{Généralité}
\begin{itemize}
    \item Fonction de perte : quantifie l'erreur associé à une décision. Erreur simple : A chaque fois qu'on se trompe, on compte 1 : 0-1 loss
    \item Risque : Proba de se tromper, $ R(y_i | x) = \sum_{j}^{} l(y_i, y_j)P(y_j | x)$ = Moyenne de la Loss pondéré par les probas à post
    \item Risque continue ? : $ R(f) = \int_{x \in \mathcal{X}}^{}R(f(x)|x)p(x) dx  $ ($ p(x) = ??? $ ) = Esperance du X sur notre domaine continue
    \item iso-contours == courbe de niveau
    \item Une epoque = on a vu une fois tous les exemples dans le gradient
    \item Hinge-loss = $ \max (0, 1 - y f_w(x)) $ \begin{itemize}
        \item the Hinge loss penalizes predictions y < 1, corresponding to the notion of a margin in a support vector machine.
        \item When $ y $ and $ f_w(x) $ have the same sign (meaning y predicts the right class) and $ \left| f_w(x) \right| \geq 1 $ , the hinge loss $ = 0 $  When they have opposite signs, the hinge loss increases linearly with $ f_w(x) $  and similarly if $ \left| f_w(x) \right| \geq 1 $, even if it has the same sign (correct prediction, but not by enough margin).
    \end{itemize}
    \item Lorsque les données sont de petites dimensions, le risque de sur-apprentissage est plus petit.
    \item Lors d'une batch de gradient, il n'est pas nécessaire de mélanger les exemples car tous les exemples sont utilisés dans chaque mise à jour de poids. VS En général, il est recommandé de mélanger les exemples lors d'une descente de gradient stochastique (SGD) afin de garantir une convergence plus rapide et une meilleure généralisation.
    \item $ \left\| x \right\|^2 = x^T x $ 
\end{itemize}

\subsection{Descente de gradient}
Point initial $ x_0 $ et $ \epsilon \geq 0 $.
\begin{enumerate}
    \item Calcul de $ \nabla f(x_k) $ 
    \item Test d'arret : si $ \left\| \nabla f(x_k) \right\| \leq \epsilon  $ 
    \item Calcul d'un pas $ \alpha _k > 0 $ par une règle de recherche linéaire en $ f $ en $ x_k $ le long de la direction $ - \nabla f(x_k) $ 
    \item Nouvel itéré : $ x_{k+1} = x_k - \alpha _k \nabla f(x_k) $ 
\end{enumerate}

\subsection{Dérivé des matrices}
\begin{align*}
    \frac{\partial (vMv)}{\partial v} &= (M + M^T)V = 2MV \text{ si } M \text{ symétrique} \\
    \frac{\partial (v^T a)}{\partial v} &= \frac{\partial (a^T v)}{\partial v} = a \\
    \frac{\partial (\log_{} \det M)}{\partial v} &= M^{-1}\\
    \frac{\partial (Tr(AM))}{\partial v} &= A
\end{align*}

\subsection{Multiplicateur de Lagrange}
Soit $ f(x) $ fonction à optimiser, sous $ g(x) = 0 $ contraintes d'égalités. On pose : $ \mathcal{L}(x,\lambda) = f(x) - \lambda  (x)$ nouvelle fonction de plus grande dimension à optimiser comme on a l'habitude en annulant le gradient : $ \nabla _x \mathcal{L}(x,\lambda ) = 0 $. Légitiment on doit vérifier si le point est un min ou un max ou un point selle avec la matrice hessienne mais osef.

\subsection{KKT \& contrainte d'inégalité}
Version complete de Lagrange : Fonction objectif $ f $, $ g_i $ contrainte d'égalité, $ h_j $ contrainte d'inégalité tel que $ h_i(x) \leq 0 $. Fonction duale : $ \mathcal{L} (x, \lambda , \mu) = f(x) - \sum_{i=1}^{p} \lambda _i g_i(x) - \sum_{j=1}^{q} \mu _j h_j(x)$. Les condition KKT pour un point $ x^* $ extremum sont (aka résoudre le système)
\[
    \begin{cases}
    \nabla \mathcal{L}(x^*, \lambda^* , \mu^* ) = 0 \\
    \mu ^*_j \leq 0, j = 1, \dots, q \\
    \mu ^*_j h_j(x^*) = 0, j =1, \dots, q
    \end{cases} 
.\]


\section{Arbre de décision}
Algo général : 
\begin{enumerate}
    \item Déterminer la meilleure caractéristique dans l'ensemble de données d'entraînement.
    \item Diviser les données d'entraînement en sous-ensembles contenant les valeurs possibles de la meilleure caractéristique.
    \item Générez de manière récursive de nouveaux arbres de décision en utilisant les sous-ensembles de données créés.
    \item Lorsqu'on ne peut plus classifier les données, on s'arrête.
\end{enumerate}
Méthode de division des données : On vas utiliser l'entropie 
\begin{defn}[Entropie]   
    \href{https://www.youtube.com/watch?v=YtebGVx-Fxw}{Origine de la formule de l'entropie}
    Soit $ X $ une variable aléatoire pouvant prendre $ n $ valeurs $ x_i $ 
    \[
        H(X) = - \sum_{i=1}^{n}P(X = x_i)\log_{} (P(X = x_i))
    .\]
    Mesure l'homogénéité d'un dataset. C'est également la moyenne de la suprise (voir la vidéo)
\end{defn}
\begin{defn}[Gain d'information]
    Mesure la réduction expects de l'entropie causé par le partitioning des exemples.

    En faisant un test $T$ sur un des attributs, on obtient deux partitions d'exemples de $X$ : $X_1$ qui vérifie le test et $X_2$ qui ne vérifie pas le test (resp. $Y_1$ et $Y_2$).
    \[
        H(Y|T) = \frac{\left| X_1 \right| }{\left| X \right| } H(Y_1) + \frac{\left| X_2 \right| }{\left| X \right| } H(Y_2)
    .\]
    Gain d'information : 
    \[
        I(T, Y) = H(Y) - H(Y|T)
    .\]
    \textbf{On veut maximiser le gain d'information par le split} $ \Leftrightarrow $ minimiser $ H(Y|T) $ 
\end{defn}

\section{KNN}
\begin{itemize}
    \item Prendre les $ K $ plus proche voisin pour classifier 
    \item $ K $ petit == noisy and subject to the effects of outliers == overfitting ? 
    \item $ K $ grand == underfitting
\end{itemize}

\section{Classfieur bayesien}
On a : 
\begin{itemize}
    \item $ P(y) $ fréquence des classe dans le dataset
    \item $ P(x|y) $ les points de notre jeux de donnée. Graphiquement : les points coloriés
\end{itemize}
On cherche : 
\[
    \arg \max _y P(y|x) = \arg \max _y \frac{P(x|y) P(y)}{P(x)}
.\]
Naive Bayes : indépendance des dimensions de $ x $, on peut développer le $ P(x|y) = P(x_1|y) \dots P(x_d|y) $.

Puis rapport de vraisemblance \textbf{en utilisant le risque} pour prendre la décision.

Remarque : 
\begin{itemize}
    \item Classifier bayésien = le classifier qui minimise le risque = le meilleurs classifieur possible
    \item Classifier optimal car minimise l'erreur car en choisissant la plus grande proba, on peut pas réduire $ 1 - P(y | x) $ qui est déjà le plus grand possible  
    \item $ P(x) $ difficile à calculer = répartition des points dans l'espace, dans le graph 2d non colorié. En général très petit, uniquement utile pour générer des données, pas pour faire l'argmax (aka classifier).
\end{itemize}
Autre truc important : 
\begin{itemize}
    \item On utilise classiquement une 0-1 loss
    \item Frontière de décision : $ \frac{R(+|x)}{R(-|x)} > 1 $  $\rightarrow$ Permet de prendre en compte les coûts asymétriques des classes. Forme dans $ \mathbb{R}^2 $ : cercle
\end{itemize}

\section{Estimation de densité}
\subsection{Par histogramme}

\begin{defn}[Estimation par histogramme]
    Soit $ Y $ une v.a.r nombre de point tombant dans un bin : $ Y \sim \mathcal{B}(n, p_b V)$. On a donc $ E(Y) = n p_b V \Leftrightarrow p_b = \frac{k}{nV} $. 
    \begin{itemize}
        \item Cas discret : Comptage dans chaque classe puis normalisation par le nombre d'exemple $ N $ $\rightarrow p_b = \frac{k}{nV} = \frac{\text{Nb dans le bin}}{\text{nb d'ech tot }*\text{ Volume d'un bin}}$,
        \item Cas continue : Discrétisation des valeurs puis comptage et normalisation
    \end{itemize}
\end{defn}
Importance de la discrétisation : \begin{itemize}
    \item Petit $\rightarrow$ sur-apprentissage, trou dans l'histogramme
    \item Trop grand $\rightarrow$ sous-apprentissage
\end{itemize}
Limite : \begin{itemize}
    \item Grande dimension $\rightarrow$ Perte de sens exponentiel (3 ou 4 max)
    \item Effet de bord : petit changement dans les bins, gros changement d'estimation. 
\end{itemize}
$\rightarrow$ Solution : Estimation par noyaux

\subsection{Estimation de densité par noyaux}
\begin{figure}[htbp]
    \centering
    \includegraphics*[width=\textwidth]{./fig1.png}
    \caption{Intuition de l'estimation par noyaux}
    \label{intuition_noyaux}
\end{figure}
Intuition figure \ref*{intuition_noyaux} : Plutôt que de décider d'une discrétisation a priori, l'estimation est faîte en centrant une fenêtre autour du point d'intérêt $ x_0 $  (dans un espace de dimension $d$) à posteriori. $\rightarrow$ Problème : pas continue (si on bouge la boite et qu'un point rentre dedans, ça fait faire un saut à la fonction)

\subsubsection{Fenêtre de Parzen}
On combine la solution précédente avec une densité/noyaux. Classiquement Gaussien. pour obtenir un truc lisse et continue
\begin{defn}[Fenêtre de Parzen]
    Soit $ (x_1, \dots, x_N) \sim f $ iid 
    \[
        \hat{f}_h(x) = \frac{1}{N*h} \sum_{i=1}^{N}K (\frac{x - x_i}{h})
    .\]
    Avec $ K $ le noyaux \textbf{centrée et réduit sur $ x $ }, souvent une fonction gaussienne. Si c'est une fonction rectangle ça fonctionne aussi. Puis y'a plein d'autre noyaux possible.
\end{defn}

\section{Regression Linéaire}
\begin{itemize}
    \item MSE : $ (XW - Y)^T (XW - Y) = W^T X^T X W - (Y^T X W)^T - YXW + Y^TY $ 
    \item \begin{align*}
        \nabla _W MSE &= 2X^TXW - X^T Y - Y^T X  \\
            &= 2X^TXW - X^T Y - X^TY  \text{ car } \lambda \in \mathbb{R}, \lambda ^T = \lambda \\
            &= 2X^T (XW - Y) = 0 \\
            &\Leftrightarrow W = (X^TX)^{-1} X^T Y
    \end{align*}
    \item Sinon descente de gradient
\end{itemize}

\section{Régression Logistique}
\begin{itemize}
    \item On peut pas utiliser la MSE car distance à la frontière de décision peut être très grande pour un point qui est très très très certainement dans une classe
    \item On vas plutot essayer de modéliser la confiance qu'on a dans la classif d'un point $\rightarrow$ Proba : $ p(y=1|x) = \mu (x) $ 
    \item Modélisation de cette proba par un truc linéaire qu'on projette entre 0 et 1 avec la sigmoide ou tanh  
    \item On remarque que le log ratio : $ \log_{} \frac{\mu (x)}{1 - \mu (x)} = f_w(x) $ pour la sigmoide 
    \item Pas de solution analytique à la log vraiss : descente de gradient
\end{itemize}

\section{Perceptron}
\begin{itemize}
    \item $ f_w(x) = <x, w> $, décision : $ sign(f_w(x)) $  
    \item Hinge-loss = $ \max (0, -y f_w(x)) $, vaut 0 quand bonne prédiction
    \item gradient Hinge loss 
    \[
        \nabla H_w = \begin{cases}
        0 &\text{ si } -y x w < 0\\
        -yx &\text{ sinon}\\
        \end{cases} 
    .\]
    \item into descente de gradient
    \item le vecteur de poids $ w $ est normal à l'hyperplans de la séparatrice, $ <w, x> $ mesure l'angle entre les deux vecteurs, maj : on fait bouger l'hyperpaln en fct de cette angle 
\end{itemize}
Théorème de convergence : si
\begin{itemize}
    \item $ \exists R, \forall x \left\| x \right\| \leq R $ 
    \item Les données peuvent être séparées avec une marge p
    \item L'ensemble d'apprentissage est présenté au perceptron un nombre suffisant
    de fois
\end{itemize}
Alors : après au plus $ \frac{R^2}{p^2} $ correction, l'algo converge


\section{SVM}
\begin{itemize}
    \item Donnée non linéaire $\rightarrow$ Projection, dim ++ $\rightarrow$ Attention sur apprentissage + quel dim choisir $\rightarrow$ SMV do this auto
    \item Résous le problème de l'unicité de la solution également
    \item Maximiser la marge $ \gamma  \Leftrightarrow $ minimiser $ \left\| w \right\| $ sous la contrainte $ \forall i, (wx^i + b)y^i \geq 1 $ par des calculs obscures ($ \geq 1 $ car on veut que la distance entre la droite de régression et ces deux marges soit supérieur 1)
    \item Prise en compte des erreurs : \begin{itemize}
        \item $ \xi  $ variable de débordement par rapport à sa marge pour chaque point mal classé $\rightarrow$ Raison obscure $\rightarrow \xi = \max (0, 1 - (wx^i + b) y^i) $ Hinge loss
        \item On avait $\min ||w||^2$ maintenant $\min ||w|||^2 + K \sum_{}^{}\xi $ avec $K$ hyper param nombre d'erreur
    \end{itemize}
    \item Optimisation avec lagrangien cas simple
    \[
        \begin{cases}
        & \min _{w,b} \frac{1}{2}\left\| w \right\| ^2\\
        &\text{s.c } y^i (wx^i + b) \geq 1\\
        \end{cases} 
        \Leftrightarrow L(w, b, \alpha) = \frac{1}{2} \left\| w \right\| ^2 - \sum_{i}^{}\alpha _i (y^i (wx^i + b) - 1)
    .\]
    \item Optimisation avec Langrangien cas complexe 
    \item \begin{align*}
        &\begin{cases}
            &\min _{w,b} \frac{1}{2}\left\| w \right\| ^2 + K \sum_{i}^{}\xi _i \\
            &\text{s.c. } y^i (wxî + b) \geq 1 - \xi _i, \xi _i \geq 0 \\
        \end{cases} \\ 
        &\Leftrightarrow \mathcal{L}(w, b, \alpha , \nu ) = \frac{1}{2}\left\| w \right\| ^2 + K \sum_{i}^{} \xi _i - \sum_{i}^{}\alpha _i (y^i(w x î + b) + \xi _i - 1) - \sum_{i}^{}\nu _i \xi _i 
    \end{align*}
        
\end{itemize}
Kernel Tricks : 
\begin{itemize}
    \item Kernel Function : $ k(x,y) = <\phi (x), \phi(y)> $
    \item Mesure la similarité entre 2 objets \begin{itemize}
        \item -- = vecteur opposé = éloigné
        \item = 0 = produit orthogonal = éloigné 
        \item ++ = vecteur aligné = proche
    \end{itemize}
    \item Stable pas addition, multiplication, composition avec $ f $ polynome, exponentiel
    \item La complexité de calcul d'un noyau polynomial est linéaire par rapport $d$ le degré du polynome. Mais pas la dimensionnalité de la projection
    \item 
\end{itemize}

Généralité SVM 
\begin{itemize}
    
    \item  The support vectors are the data points that lie on the margin, which is the region between the decision boundary and the closest data points of each class. Support vectors are critical in SVM because they determine the location and orientation of the decision boundary. All other data points that are not support vectors are not used to construct the decision boundary, which means that SVM is robust to noise and outliers in the data.
    \item La taille de la marge est un hyper-paramètre important : marge grande == underfitting // marge petite == overfitting (séparation linéaire plus proche des points, moins centrée)
\end{itemize}

\section{Réseau de neurone}

\section{Non supervisé}

\section{Théorie de l'apprentissage}
\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=.65\textwidth]{biais-variance.png}
    \caption{Variance = estimation error, biai = approximation error}
    \label{<label>}
\end{figure}
\begin{itemize}
    \item Algorithme universellement consistant : risque de la fonction apprise converge vers le risque bayesien. Si on peut overfit sur n'importe quel jeu de donnée (comme un arbre de décision ou un réseau de neurone avec nombre de paramètre infiny).
    \item Biais : risque lié à la restriction de la taille de l'ensemble de fonction où on cherche la fonction optimal
    \item Variance : Risque lié à notre set de train et au capacité de généralisation du modèle
    \item Compromis biais variance : Si on augmente la taille de $\mathcal{F}$ alors on réduit le biai potentiel mais on augmente la variance potentielle
    \item Pour un ensemble discret $ \mathcal{F} $, on peut trouver une vitesse de convergence logarithmique par rapport aux nombre de fonction $ \left\| \mathcal{F} \right\|  $ 
    \item Pour un ensemble infiny de fonction : de même mais utilise la VC-dimension de $ \mathcal{F} : VC(\mathcal{F}) $ 
    \item VC-dimension : \begin{itemize}
        \item Un ensemble de points est shattered (pulvérisé) par un espace de fonction si pour tout partitionnement des points en deux ensembles il existe une fonction qui sépare les deux partitions.
        \item La VC-dimension (Vapnik-Chervonenkis) de F sur un espace de données $ \mathcal{X} $  est la taille du plus grande ensemble fini de points de $ \mathcal{X} $  pulvérisé par $ \mathcal{F} $ .
        \item Fonctions linéaires : en dimension $ d $, VC-dimension de $ d + 1 $
        \item le nombre d'exemples doit être linéaire en fonction de $ VC(\mathcal{F}) $ 
        \item \textbf{VC-dim grande} = modèle flexible = fit/overfit sur données complexe = plus sensible au bruit // \textbf{VC-dim faible} = fonction plus "général" = moins d'overfit mais moins de performance aussi = plus robust au bruit 
    \end{itemize}
\end{itemize}

\section{Bagging et Boosting}
On veut pouvoir combiner des classfiers, ils doivent être indépendant. Comment les rendre indépendant entre eux ? Deux solutions

\subsection{Bagging}
\begin{itemize}
    \item Sous échantillon du train, tirage avec remise
    \item Random forest = arbre de décision + bagging = randomisation du support de décision puis moyenne de tous les noeuds des arbres 
    \item Chacun vas un peu apprendre une partie de l'espace, lié au hasard, puis les décision se moyenne
    \item Très efficace à grande dimensionnalité
\end{itemize}

\subsection{Boosting}
\begin{itemize}
    \item Correction des erreurs faite en $ n-1 $ en $ n $ avec une pondération des exemples dans la loss
    \item Poids uniforme $\rightarrow$ On augmente le poids où il a fait des erreurs et baisse les poids des bons exemples
    \item $\rightarrow$ combinaison des frontière de décision
    \item Beaucoup d'overfit $\rightarrow$ garder des arbres de décision à faible profondeur
    \item Ecrit sous une forme gloutone == descente de gradient pour les arbre de décision 
    \item AdaBoost : Si l'erreur est supérieur à 0.5 on a pas su améliorer la classif alors on arrete
\end{itemize}


\section{Reinforcement Learning}
Policy itération \begin{itemize}
    \item $ V^\pi (s) = E[G_t | S_t = s] = E[ \sum_{k \geq 0}^{} \gamma ^k R_{t+k}] $ avec $ R_{t+k} $ les récompenss obtenues à l'état $ t+k $. $ V^\pi $ représente la moyenne des récompenses possible dans le futur, futur plus où moins proche réglé par $ \gamma  $ 
    \item Par l'equation de Bellman
    \[
        V_{k+1}(s) = (T^\pi V_k)(s) = \sum_{a}^{}\pi (a | s) \sum_{s'}^{}P(s' | s, a) [r(s, a, s') + \gamma V_k(s')]
    .\]
    \item Une application répété de l'opérateur de Bellman $ T^\pi  $ fait converger $ V_k $ pour évaluer la policy $ \pi  $ 
    \item Puis mise à jour de la politique pour chaque état en choisisant l'action qui maximise $ V^k(s) $ si j'ai bien compris. 
    \[
        \pi _{k+1}(s) = argmax_a \sum_{s'}^{} P(s' | s, a) [ r(s, a, s') + \gamma V^{k}(s')]
    .\]
    \item L'étape d'évaluation de la policy est couteuse. On peut la skip en évaluant directement chaque action pendant l'entrainement, ça fait sauter la première somme dans la formule donnant 
    \[
        V^*(s) = \max _\pi V^\pi (s) = \max _a \sum_{s'}^{} P(s' | s, a)[r(s, a, s') + \gamma V^*(s')]
    .\]
\end{itemize}


\paragraph*{Application} : 

Evaluation d'une politique $ \pi  $ 
\begin{itemize}
    \item Simplifier et écrire la formule pour notre MDP, en particulier pour les états stochastiques ou non, la somme avec la proba peut disparaitre où bien se réduire.
    \item Tableau avec les états en colonne et les $ V^\pi_k (s) $ en ligne, on vas faire augmenter $ k $ 
    \item Appliquer la formule à chaque étape
\end{itemize}
Exemple d'une formule simplifiée : Voir TD9 Question 2 avec une marche aléatoire. 
\[
    \pi (gauche | s_i) (r(s_i, g, s_{i-1}) + \gamma V_k^\pi (s_{i-1})) + \pi (droite | s_i) (r(s_i, d, s_{i+1}) = \gamma V_k^\pi (s_{i+1}))
.\]

Mise à jour de $ \pi $ : TODO

value iterattion !!!! je crois que c'est juste une maj differente a partie de la valeur des etat, résumer \href{https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration}{ça}  


\end{document}