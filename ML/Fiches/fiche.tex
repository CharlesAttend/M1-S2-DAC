\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein's Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Fiche ML}
\author{Charles Vin}
\date{S2-2023}

\begin{document}
\maketitle
\section{Généralité}
\begin{itemize}
    \item Fonction de perte : quantifie l'erreur associé à une décision. Erreur simple : A chaque fois qu'on se trompe, on compte 1 : 0-1 loss
    \item Risque : Proba de se tromper, $ R(y_i | x) = \sum_{j}^{} l(y_i, y_j)P(y_j | x)$ = Moyenne de la Loss pondéré par les probas 
    \item Risque continue ? : $ R(f) = \int_{x \in \mathcal{X}}^{}R(f(x)|x)p(x) dx  $ ($ p(x) = ??? $ ) = Esperance du X sur notre domaine continue
    \item iso-contours == courbe de niveau
    \item Une epoque = on a vu une fois tous les exemples dans le gradient
    \item Hinge-loss = $ \max (0, -y f_w(x)) $ 
\end{itemize}

\section{Arbre de décision}
Algo général : 
\begin{enumerate}
    \item Déterminer la meilleure caractéristique dans l'ensemble de données d'entraînement.
    \item Diviser les données d'entraînement en sous-ensembles contenant les valeurs possibles de la meilleure caractéristique.
    \item Générez de manière récursive de nouveaux arbres de décision en utilisant les sous-ensembles de données créés.
    \item Lorsqu'on ne peut plus classifier les données, on s'arrête.
\end{enumerate}
Méthode de division des données : On vas utiliser l'entropie 
\begin{defn}[Entropie]   
    \href{https://www.youtube.com/watch?v=YtebGVx-Fxw}{Origine de la formule de l'entropie}
    Soit $ X $ une variable aléatoire pouvant prendre $ n $ valeurs $ x_i $ 
    \[
        H(X) = - \sum_{i=1}^{n}P(X = x_i)\log_{} (P(X = x_i))
    .\]
    Mesure l'homogénéité d'un dataset. C'est également la moyenne de la suprise (voir la vidéo)
\end{defn}
\begin{defn}[Gain d'information]
    Mesure la réduction expects de l'entropie causé par le partitioning des exemples.

    En faisant un test $T$ sur un des attributs, on obtient deux partitions d'exemples de $X$ : $X_1$ qui vérifie le test et $X_2$ qui ne vérifie pas le test (resp. $Y_1$ et $Y_2$).
    \[
        H(Y|T) = \frac{\left| X_1 \right| }{\left| X \right| } H(Y_1) + \frac{\left| X_2 \right| }{\left| X \right| } H(Y_2)
    .\]
    Gain d'information : 
    \[
        I(T, Y) = H(Y) - H(Y|T)
    .\]
    \textbf{On veut maximiser le gain d'information par le split} $ \Leftrightarrow $ minimiser $ H(Y|T) $ 
\end{defn}


\section{Classfieur bayesien}
On a : 
\begin{itemize}
    \item $ P(y) $ fréquence des classe dans le dataset
    \item $ P(x|y) $ les points de notre jeux de donnée. Graphiquement : les points coloriés
    \item 
\end{itemize}
On cherche : 
\[
    \arg \max _y P(y|x) = \arg \max _y \frac{P(x|y) P(y)}{P(x)}
.\]
Par indépendance des dimensions de $ x $, on peut parfois  développer le $ P(x|y) = P(x_1|y) \dots P(x_d|y) $.

Puis rapport de vraisemblance pour prendre la décision.

Remarque : 
\begin{itemize}
    \item Classifier bayésien = le classifier qui minimise le risque = le meilleurs classifieur possible
    \item Classfier optimal car minimise l'erreur car en choisissant la plus grande proba, on peut pas réduire $ 1 - P(y | x) $ qui est déjà le plus grand possible  
    \item $ P(x) $ difficile à calculer = répartition des points dans l'espace, dans le graph 2d non colorié. En général très petit, uniquement utile pour générer des données, pas pour faire l'argmax (aka classifier).
\end{itemize}

\section{Estimation de densité}
\subsection{Par histogramme}

\begin{defn}[Estimation par histogramme]
    \begin{itemize}
        \item Cas discret : Comptage dans chaque classe puis normalisation par le nombre d'exemple $ N $ 
        \item Cas continue : Discrétisation des valeurs puis comptage et normalisation
    \end{itemize}
\end{defn}
Importance de la discrétisation : \begin{itemize}
    \item Petit $\rightarrow$ sur-apprentissage, 
    \item Trop grand $\rightarrow$ sous-apprentissage
\end{itemize}
Limite : \begin{itemize}
    \item Grande dimention $\rightarrow$ Perte de sens exponentiel (3 ou 4 max)
    \item Effet de bord : petit changement dans les bins, gros changement d'estimation. 
\end{itemize}
$\rightarrow$ Solution : Estimation par noyaux

\subsection{Estimation de densité par noyaux}
\begin{figure}[htbp]
    \centering
    \includegraphics*[width=\textwidth]{./fig1.png}
    \caption{Intuition de l'estimation par noyaux}
    \label{intuition_noyaux}
\end{figure}
Intuition figure \ref*{intuition_noyaux} : Plutôt que de décider d'une discrétisation a priori, l'estimation est faîte en centrant une fenêtre autour du point d'intérêt $ x_0 $  (dans un espace de dimension $d$) à posteriori. $\rightarrow$ Problème : pas continue (si on bouge la boite et qu'un point rentre dedans, ça fait faire un saut à la fonction)

\subsubsection{Fenêtre de Parzen}
On combine la solution précédente avec une densité/noyaux. Classiquement Gaussien. pour obtenir un truc lisse et continue
\begin{defn}[Fenêtre de Parzen]
    Soit $ (x_1, \dots, x_N) \sim f $ iid 
    \[
        \hat{f}_h(x) = \frac{1}{N*h} \sum_{i=1}^{N}K (\frac{x - x_i}{h})
    .\]
    Avec $ K $ le noyaux \textbf{centrée et réduit sur $ x $ }, souvent une fonction gaussienne. Si c'est une fonction rectangle ça fonctionne aussi. Puis y'a plein d'autre noyaux possible.
\end{defn}


\section{Regression Linéaire}
\begin{itemize}
    \item MSE : $ (XW - Y)^T (XW - Y) = W^T X^T X W - (Y^T X W)^T - YXW + Y^TY $ 
    \item \begin{align*}
        \nabla _W MSE &= 2X^TXW - X^T Y - Y^T X  \\
            &= 2X^TXW - X^T Y - X^TY  \text{ car } \lambda \in \mathbb{R}, \lambda ^T = \lambda \\
            &= 2X^T (XW - Y) = 0
            &\Leftrightarrow W = (X^TX)^{-1} X^T Y
    \end{align*}
    \item Sinon descente de gradiant
\end{itemize}

\section{Régression Logistique}
\begin{itemize}
    \item On peut pas utiliser la MSE car distance à la frontière de décision peut être très grande pour un point qui est très très très certainement dans une classe
    \item On vas plutot essayer de modéliser la confiance qu'on a dans la classif d'un point $\rightarrow$ Proba : $ p(y=1|x) = \mu (x) $ 
    \item Modélisation de cette proba par un truc linéaire qu'on projette entre 0 et 1 avec la sigmoide ou tanh  
    \item On remarque que le log ratio : $ \log_{} \frac{\mu (x)}{1 - \mu (x)} = f_w(x) $ pour la sigmoide 
    \item Pas de solution analytique à la log vraiss : descente de gradient
\end{itemize}

\section{Perceptron}
\begin{itemize}
    \item $ f_w(x) = x \bullet w    $ 
    \item Hinge-loss = $ \max (0, -y f_w(x)) $, vaut 0 quand bonne prédiction
    \item gradient Hinge loss 
    \[
        \nabla H_w = \begin{cases}
        0 &\text{ si } -y x w < 0\\
        -yx &\text{ sinon}\\
        \end{cases} 
    .\]
    \item into descente de gradient
\end{itemize}

\section{SVM}
\begin{itemize}
    \item Donnée non linéaire $\rightarrow$ Projection, dim ++ $\rightarrow$ Attention sur apprentissage + quel dim choisir $\rightarrow$ SMV do this auto
    \item Maximiser la marge $ \gamma  \Leftrightarrow $ minimiser $ \left\| w \right\| $ sous la contrainte $ \forall i, (wx^i + b)y^i \geq 1 $ par des calculs obscures ($ \geq 1 $ car on veut que la distance entre la droite de régression et ces deux marges soit supérieur 1)
    \item Prise en comtpe des erreurs : \begin{itemize}
        \item $ \xi  $ variable de débordement par rapport à sa marge pour chaque point mal classé $\rightarrow$ Raison obscure $\rightarrow \xi = \max (0, 1 - (wx^i + b) y^i) $ Hinge loss
        \item On avait $\min ||w||²$ maintenant $\min ||w|||^2 + K \sum_{}^{}\xi $ avec $K$ hyper param nombre d'erreur
    \end{itemize}
    \item Optimisation avec lagrangien cas simple
    \[
        \begin{cases}
        & \min _{w,b} \frac{1}{2}\left\| w \right\| ^2\\
        &\text{s.c } y^i (wx^i + b) \geq 1\\
        \end{cases} 
        \Leftrightarrow L(w, b, \alpha) = \frac{1}{2} \left\| w \right\| ^2 - \sum_{i}^{}\alpha _i (y^i (wx^i + b) - 1)
    .\]
    
\end{itemize}
\end{document}