\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein's Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Fiche ML}
\author{Charles Vin}
\date{S2-2023}

\begin{document}
\maketitle
\section{Généralité}
\begin{itemize}
    \item Fonction de perte : quantifie l'erreur associé à une décision. Erreur simple : A chaque fois qu'on se trompe, on compte 1 : 0-1 loss
    \item Risque : Proba de se tromper, $ R(y_i | x) = \sum_{j}^{} l(y_i, y_j)P(y_j | x)$ = Moyenne de la Loss pondéré par les probas à post
    \item Risque continue ? : $ R(f) = \int_{x \in \mathcal{X}}^{}R(f(x)|x)p(x) dx  $ ($ p(x) = ??? $ ) = Esperance du X sur notre domaine continue
    \item iso-contours == courbe de niveau
    \item Une epoque = on a vu une fois tous les exemples dans le gradient
    \item Hinge-loss = $ \max (0, 1 - y f_w(x)) $ \begin{itemize}
        \item the Hinge loss penalizes predictions y < 1, corresponding to the notion of a margin in a support vector machine.
        \item When $ y $ and $ f_w(x) $ have the same sign (meaning y predicts the right class) and $ \left| f_w(x) \right| \geq 1 $ , the hinge loss $ = 0 $  When they have opposite signs, the hinge loss increases linearly with $ f_w(x) $  and similarly if $ \left| f_w(x) \right| \geq 1 $, even if it has the same sign (correct prediction, but not by enough margin).
    \end{itemize}
    \item Lorsque les données sont de petites dimensions, le risque de sur-apprentissage est plus petit.
    \item Lors d'une batch de gradient, il n'est pas nécessaire de mélanger les exemples car tous les exemples sont utilisés dans chaque mise à jour de poids. VS En général, il est recommandé de mélanger les exemples lors d'une descente de gradient stochastique (SGD) afin de garantir une convergence plus rapide et une meilleure généralisation.
    \item $ \left\| x \right\|^2 = x^T x $ 
\end{itemize}

\subsection{Descente de gradient}
Point initial $ x_0 $ et $ \epsilon \geq 0 $.
\begin{enumerate}
    \item Calcul de $ \nabla f(x_k) $ 
    \item Test d'arret : si $ \left\| \nabla f(x_k) \right\| \leq \epsilon  $ 
    \item Calcul d'un pas $ \alpha _k > 0 $ par une règle de recherche linéaire en $ f $ en $ x_k $ le long de la direction $ - \nabla f(x_k) $ 
    \item Nouvel itéré : $ x_{k+1} = x_k - \alpha _k \nabla f(x_k) $ 
\end{enumerate}

\subsection{Dérivé des matrices}
\begin{align*}
    \frac{\partial (vMv)}{\partial v} &= (M + M^T)V = 2MV \text{ si } M \text{ symétrique} \\
    \frac{\partial (v^T a)}{\partial v} &= \frac{\partial (a^T v)}{\partial v} = a \\
    \frac{\partial (\log_{} \det M)}{\partial v} &= M^{-1}\\
    \frac{\partial (Tr(AM))}{\partial v} &= A
\end{align*}

\section{Arbre de décision}
Algo général : 
\begin{enumerate}
    \item Déterminer la meilleure caractéristique dans l'ensemble de données d'entraînement.
    \item Diviser les données d'entraînement en sous-ensembles contenant les valeurs possibles de la meilleure caractéristique.
    \item Générez de manière récursive de nouveaux arbres de décision en utilisant les sous-ensembles de données créés.
    \item Lorsqu'on ne peut plus classifier les données, on s'arrête.
\end{enumerate}
Méthode de division des données : On vas utiliser l'entropie 
\begin{defn}[Entropie]   
    \href{https://www.youtube.com/watch?v=YtebGVx-Fxw}{Origine de la formule de l'entropie}
    Soit $ X $ une variable aléatoire pouvant prendre $ n $ valeurs $ x_i $ 
    \[
        H(X) = - \sum_{i=1}^{n}P(X = x_i)\log_{} (P(X = x_i))
    .\]
    Mesure l'homogénéité d'un dataset. C'est également la moyenne de la suprise (voir la vidéo)
\end{defn}
\begin{defn}[Gain d'information]
    Mesure la réduction expects de l'entropie causé par le partitioning des exemples.

    En faisant un test $T$ sur un des attributs, on obtient deux partitions d'exemples de $X$ : $X_1$ qui vérifie le test et $X_2$ qui ne vérifie pas le test (resp. $Y_1$ et $Y_2$).
    \[
        H(Y|T) = \frac{\left| X_1 \right| }{\left| X \right| } H(Y_1) + \frac{\left| X_2 \right| }{\left| X \right| } H(Y_2)
    .\]
    Gain d'information : 
    \[
        I(T, Y) = H(Y) - H(Y|T)
    .\]
    \textbf{On veut maximiser le gain d'information par le split} $ \Leftrightarrow $ minimiser $ H(Y|T) $ 
\end{defn}

\section{KNN}
\begin{itemize}
    \item Prendre les $ K $ plus proche voisin pour classifier 
    \item $ K $ petit == noisy and subject to the effects of outliers == overfitting ? 
    \item $ K $ grand == underfitting
\end{itemize}

\section{Classfieur bayesien}
On a : 
\begin{itemize}
    \item $ P(y) $ fréquence des classe dans le dataset
    \item $ P(x|y) $ les points de notre jeux de donnée. Graphiquement : les points coloriés
\end{itemize}
On cherche : 
\[
    \arg \max _y P(y|x) = \arg \max _y \frac{P(x|y) P(y)}{P(x)}
.\]
Naive Bayes : indépendance des dimensions de $ x $, on peut développer le $ P(x|y) = P(x_1|y) \dots P(x_d|y) $.

Puis rapport de vraisemblance \textbf{en utilisant le risque} pour prendre la décision.

Remarque : 
\begin{itemize}
    \item Classifier bayésien = le classifier qui minimise le risque = le meilleurs classifieur possible
    \item Classifier optimal car minimise l'erreur car en choisissant la plus grande proba, on peut pas réduire $ 1 - P(y | x) $ qui est déjà le plus grand possible  
    \item $ P(x) $ difficile à calculer = répartition des points dans l'espace, dans le graph 2d non colorié. En général très petit, uniquement utile pour générer des données, pas pour faire l'argmax (aka classifier).
\end{itemize}
Autre truc important : 
\begin{itemize}
    \item On utilise classiquement une 0-1 loss
    \item Frontière de décision : $ \frac{R(+|x)}{R(-|x)} > 1 $  $\rightarrow$ Permet de prendre en compte les coûts asymétriques des classes. Forme dans $ \mathbb{R}^2 $ : cercle
\end{itemize}

\section{Estimation de densité}
\subsection{Par histogramme}

\begin{defn}[Estimation par histogramme]
    Soit $ Y $ une v.a.r nombre de point tombant dans un bin : $ Y \sim \mathcal{B}(n, p_b V)$. On a donc $ E(Y) = n p_b V \Leftrightarrow p_b = \frac{k}{nV} $. 
    \begin{itemize}
        \item Cas discret : Comptage dans chaque classe puis normalisation par le nombre d'exemple $ N $ $\rightarrow p_b = \frac{k}{nV} = \frac{\text{Nb dans le bin}}{\text{nb d'ech tot }*\text{ Volume d'un bin}}$,
        \item Cas continue : Discrétisation des valeurs puis comptage et normalisation
    \end{itemize}
\end{defn}
Importance de la discrétisation : \begin{itemize}
    \item Petit $\rightarrow$ sur-apprentissage, trou dans l'histogramme
    \item Trop grand $\rightarrow$ sous-apprentissage
\end{itemize}
Limite : \begin{itemize}
    \item Grande dimension $\rightarrow$ Perte de sens exponentiel (3 ou 4 max)
    \item Effet de bord : petit changement dans les bins, gros changement d'estimation. 
\end{itemize}
$\rightarrow$ Solution : Estimation par noyaux

\subsection{Estimation de densité par noyaux}
\begin{figure}[htbp]
    \centering
    \includegraphics*[width=\textwidth]{./fig1.png}
    \caption{Intuition de l'estimation par noyaux}
    \label{intuition_noyaux}
\end{figure}
Intuition figure \ref*{intuition_noyaux} : Plutôt que de décider d'une discrétisation a priori, l'estimation est faîte en centrant une fenêtre autour du point d'intérêt $ x_0 $  (dans un espace de dimension $d$) à posteriori. $\rightarrow$ Problème : pas continue (si on bouge la boite et qu'un point rentre dedans, ça fait faire un saut à la fonction)

\subsubsection{Fenêtre de Parzen}
On combine la solution précédente avec une densité/noyaux. Classiquement Gaussien. pour obtenir un truc lisse et continue
\begin{defn}[Fenêtre de Parzen]
    Soit $ (x_1, \dots, x_N) \sim f $ iid 
    \[
        \hat{f}_h(x) = \frac{1}{N*h} \sum_{i=1}^{N}K (\frac{x - x_i}{h})
    .\]
    Avec $ K $ le noyaux \textbf{centrée et réduit sur $ x $ }, souvent une fonction gaussienne. Si c'est une fonction rectangle ça fonctionne aussi. Puis y'a plein d'autre noyaux possible.
\end{defn}

\section{Regression Linéaire}
\begin{itemize}
    \item MSE : $ (XW - Y)^T (XW - Y) = W^T X^T X W - (Y^T X W)^T - YXW + Y^TY $ 
    \item \begin{align*}
        \nabla _W MSE &= 2X^TXW - X^T Y - Y^T X  \\
            &= 2X^TXW - X^T Y - X^TY  \text{ car } \lambda \in \mathbb{R}, \lambda ^T = \lambda \\
            &= 2X^T (XW - Y) = 0 \\
            &\Leftrightarrow W = (X^TX)^{-1} X^T Y
    \end{align*}
    \item Sinon descente de gradient
\end{itemize}

\section{Régression Logistique}
\begin{itemize}
    \item On peut pas utiliser la MSE car distance à la frontière de décision peut être très grande pour un point qui est très très très certainement dans une classe
    \item On vas plutot essayer de modéliser la confiance qu'on a dans la classif d'un point $\rightarrow$ Proba : $ p(y=1|x) = \mu (x) $ 
    \item Modélisation de cette proba par un truc linéaire qu'on projette entre 0 et 1 avec la sigmoide ou tanh  
    \item On remarque que le log ratio : $ \log_{} \frac{\mu (x)}{1 - \mu (x)} = f_w(x) $ pour la sigmoide 
    \item Pas de solution analytique à la log vraiss : descente de gradient
\end{itemize}

\section{Perceptron}
\begin{itemize}
    \item $ f_w(x) = <x, w> $, décision : $ sign(f_w(x)) $  
    \item Hinge-loss = $ \max (0, -y f_w(x)) $, vaut 0 quand bonne prédiction
    \item gradient Hinge loss 
    \[
        \nabla H_w = \begin{cases}
        0 &\text{ si } -y x w < 0\\
        -yx &\text{ sinon}\\
        \end{cases} 
    .\]
    \item into descente de gradient
    \item le vecteur de poids $ w $ est normal à l'hyperplans de la séparatrice, $ <w, x> $ mesure l'angle entre les deux vecteurs, maj : on fait bouger l'hyperpaln en fct de cette angle 
\end{itemize}
Théorème de convergence : si
\begin{itemize}
    \item $ \exists R, \forall x \left\| x \right\| \leq R $ 
    \item Les données peuvent être séparées avec une marge p
    \item L'ensemble d'apprentissage est présenté au perceptron un nombre suffisant
    de fois
\end{itemize}
Alors : après au plus $ \frac{R^2}{p^2} $ correction, l'algo converge


\section{SVM}
\begin{itemize}
    \item Donnée non linéaire $\rightarrow$ Projection, dim ++ $\rightarrow$ Attention sur apprentissage + quel dim choisir $\rightarrow$ SMV do this auto
    \item Résous le problème de l'unicité de la solution également
    \item Maximiser la marge $ \gamma  \Leftrightarrow $ minimiser $ \left\| w \right\| $ sous la contrainte $ \forall i, (wx^i + b)y^i \geq 1 $ par des calculs obscures ($ \geq 1 $ car on veut que la distance entre la droite de régression et ces deux marges soit supérieur 1)
    \item Prise en compte des erreurs : \begin{itemize}
        \item $ \xi  $ variable de débordement par rapport à sa marge pour chaque point mal classé $\rightarrow$ Raison obscure $\rightarrow \xi = \max (0, 1 - (wx^i + b) y^i) $ Hinge loss
        \item On avait $\min ||w||^2$ maintenant $\min ||w|||^2 + K \sum_{}^{}\xi $ avec $K$ hyper param nombre d'erreur
    \end{itemize}
    \item Optimisation avec lagrangien cas simple
    \[
        \begin{cases}
        & \min _{w,b} \frac{1}{2}\left\| w \right\| ^2\\
        &\text{s.c } y^i (wx^i + b) \geq 1\\
        \end{cases} 
        \Leftrightarrow L(w, b, \alpha) = \frac{1}{2} \left\| w \right\| ^2 - \sum_{i}^{}\alpha _i (y^i (wx^i + b) - 1)
    .\]
\end{itemize}
Kernel Tricks : 
\begin{itemize}
    \item Kernel Function : $ k(x,y) = <\phi (x), \phi(y)> $
    \item Mesure la similarité entre 2 objets \begin{itemize}
        \item -- = vecteur opposé = éloigné
        \item = 0 = produit orthogonal = éloigné 
        \item ++ = vecteur aligné = proche
    \end{itemize}
    \item Stable pas addition, multiplication, composition avec $ f $ polynome, exponentiel
    \item La dimensionnalité de la projection et la complexité de calcul d'un noyau polynomial est linéaire par rapport $d$ le degré du polynome.
    \item 
\end{itemize}

Généralité SVM 
\begin{itemize}
    
    \item  The support vectors are the data points that lie on the margin, which is the region between the decision boundary and the closest data points of each class. Support vectors are critical in SVM because they determine the location and orientation of the decision boundary. All other data points that are not support vectors are not used to construct the decision boundary, which means that SVM is robust to noise and outliers in the data.
    \item La taille de la marge est un hyper-paramètre important : marge grande == underfitting // marge petite == overfitting (séparation linéaire plus proche des points, moins centrée)
\end{itemize}
\end{document}