{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def forward(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    def backward(self, y, yhat):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self._parameters = None\n",
    "        self._gradient = None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        ## Annule gradient\n",
    "        self._gradient = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        ## Calcule la passe forward\n",
    "        pass\n",
    "\n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        ## Calcule la mise a jour des parametres selon le gradient calcule et le pas de gradient_step\n",
    "        self._parameters -= gradient_step*self._gradient\n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        ## Met a jour la valeur du gradient\n",
    "        pass\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcul la derivee de l'erreur\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mon premier est ... linéaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok si je comprend bien dans nos classe : \n",
    "- le paramètre `delta` c'est $\\frac{\\partial L}{\\partial z^{h+1}}$\n",
    "- backward_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y, yhat):\n",
    "        return (y - yhat)**2\n",
    "    \n",
    "    def backward(self, y, yhat):\n",
    "        return 2*(y - yhat)\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self._parameters = np.random.random((input, output))\n",
    "\n",
    "    def forward(self, X):\n",
    "        assert X.shape[1] == self.input\n",
    "        return X @ self._parameters\n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        ## Met a jour la valeur du gradient\n",
    "        self._gradient += input.T @ delta\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcul la derivee de l'erreur\n",
    "        return delta @ self._parameters.T\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
