\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}

\graphicspath{{./RI/}}

\title{Fiche RI}
\author{Charles Vin}
\date{2023}

\begin{document}
\maketitle

\section{Généralité}
\begin{itemize}
    \item RI ad-hoc : c'est ce qu'on fait : trouver parmi un ensemble d'articles ceux qui concernent un sujet spécifique.
    \item Indexation = encodage des documents avec un modèle RI
    \item Deux types d'index \begin{itemize}
        \item Index normal : Document : (terme, nombre)
        \item Index inversé : Mot : (document, nombre)
    \end{itemize}
    \item Stemming : ne garde que la racine des mots, un peu moche 
    \item Lematization : retour vers un mot complet
    \item Strategie de recherche : \begin{itemize}
        \item Problème : On a beaucoup de doc, on cherche les K premiers
        \item Deux strat  pour index inversé : \begin{itemize}
            \item TAAT : traiter les terme un par un, fonctionne bien sur des petits corpus où il y a une grande différence de score
            \item DAAT : traiter les doc un par un, plus efficace pour les grandes collections, plus 
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Métrique}
Précision et rappel
\begin{itemize}
    \item Recall : Pourcentage de documents pertinents renvoyés parmi tous ceux qui sont pertinents. Utilisé quand les faux négatifs sont couteux (exemple : le médical).
    \[
        \frac{\left\| R \cap P \right\| }{\left\| P \right\| }
    .\]
    Avec $ R $ l'ensemble des documents renvoyés et $ P $ les documents pertinents. 

    \item Précision : Pourcentable de documents pertinents renvoyés parmis ceux renvoyés. Utilisé quand les faux positif sont couteux (exemple : la RI).
    \[
        \frac{\left\| R \cap P \right\| }{\left\| R \right\| }
    .\]
    Avec $ R $ l'ensemble des documents renvoyés et $ P $ les documents pertinents.

    \item C'est un compromi, augmenter l'un fait baisser l'autre
\end{itemize}
\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=.5\textwidth]{precision_recall_conf_matrix.png}
    \caption{En rouge la précision, en jaune le rappel}
    \label{fig:precision_recall}
\end{figure}

\section{Loi de Zipf}
Stipule que la fréquence d'occurrence d'un mot est inversement proportionnelle à celle de son rang. Le 1er mot est environ 2 fois plus fréquent que le 2nd qui est 2 fois plus fréquent que le 3e etc.. 
\[
    \frac{ \frac{1 }{r^s}}{ \sum_{n=1}^{N} \frac{1}{N}} \approx \frac{1}{r^s}
.\]
Avec $ r $ le rang, $ N $ la taille du corpus et $ s $ un paramètre spécifique au corpus

\section{Loi de Heaps}
Lien entre le nombre de mots distinct et le nombre de mots : \begin{itemize}
    \item les nouveaux mots apparaissent moins fréquenmment quand le vocabulaire croît. 
    \item La taille du vocabulaire n'a pas de borne supérieure (nom propres, erreur de typo)
\end{itemize} 
\[
    V = K n ^\beta 
.\]
Avec $ V $ taille du vocabulaire, $ N $ taille du texte, $ K, \beta  $ paramètre spécifique du texte.

\section{TF-IDF}
\begin{itemize}
    \item Term Frequency : Une pondération locale, nombre occurrences du terme dans le document
    \item Inverse Document frequency : Une pondération globale, fréquence inverse décroit vers 0 si le terme apparait dans tous les documents $ \frac{N}{df(t_i)} $ avec $ df $ nombre de documents contenant le terme. 
    \item TF-IDF : $ tf * idf $, il existe plein de variance
\end{itemize}


\section{Modèle booléen}
\section{Modèle vectoriel}
\section{Modèle probabiliste}
Soit $ R $ une v.a.r binaire pour un dociment $ d $ est pertinent pour la question $ q $. 
\begin{itemize}
    \item On cherche la proba $ P(R | q,d) $ que le document soit pertinent pour la question et le document.
    \item Par bayes on tombe sur $ P(d | R, q) P(R | q ) $.
    \item Un document se décompose en terme \textbf{indépendant} de cette manière  $d : (\bigwedge_{t \in d} ) \wedge (\bigwedge_{t \not \in d} t \not \in d)$
    \item $ \prod_{t \in d}^{} P(t \in d | R, q) \prod_{t \not\in d} P(t \not \in d | R, q) P( R | q) $
    \item Et on peut estimer la proba qu'un terme apparaisse dans un document $ p_t = P(t \in d | R, q), 1 - p_t = P(t \not\in d | R, q) $.
    \item Finalement on peut développer un peu avec un tricks sur le produit et virer les constantes\begin{align*}
        P(R | d,q) &= P(R|q) \prod_{t \in d} p_t \prod_{t \not\in d} 1 - p_t \\
                &= P(R | q) \prod_{t \in d}^{} p_t \frac{1}{1 - p_t} \prod_{t \in \mathcal{T}}^{} 1 - p_t \\ 
                &\propto \prod_{t \in d}^{} p_t \frac{1}{1 - p_t}
    \end{align*}
    \item De base le model pose $ \frac{P(R | q, d)}{P(\not R | q, d)} $, même développement qu'au dessus, puis passage au log pour obtenir un score 
    \[
        s(q,d) = \sum_{t \in q \sqcap d}^{} \log \frac{p_t (1 - u_t)}{u_t (1 - p_t)}
    .\]
    \item On estime les proba par max vraissemblance qui donne juste la fréquence des termes
    \item On peut intégré un prior sur $ P(d) $ mais je sais honnetement pas à quelle étape : longueur du doc, longueur moyenne des mots, date, nombre de lien, pagerank
\end{itemize}

\subsection{BM25/Okapi}
Si $ t \not\in d \Leftrightarrow TF_t = 0 $ puis avec une modelisation de poisson sur la valeur de ce terme, on retombe sur la formule du coef BM25. Woah qu'est ce que c'est que ce trucs c'est le futur à estimer les param de la poisson

\section{Modèle de langue }
Basé sur l'idée que pour faire une recherche on imagine les mots que le documents pertinent vas contenir. Quel est la proba que le document soit généré par le même modèle de langue que le document.

\[
    p(t_1, \dots, t_n) = \sum_{t}^{}tf(t)\log_{} P(t | \theta _{Md}) = \sum_{t}^{}tf(t)\log_{} p_t
.\]
Par max vraissemblance + langragien  $ p_t = \frac{tf_(t)}{\sum_{t}^{}tf(t)} $ . C'est très très flou dans le diapo.

Dans le cas où un mot de la requête n'apparrait pas dans le document, score = 0 $\rightarrow$ Lissage des probas = modèle de
mélange multinomial entre la distribution des termes dans le
document et la distribution des termes dans la collection = Dirichlet ou Jelinek-Mercer

\section{Reformulation de requete}
\subsection{Revelance feedback}
Recalculer les poids des document en fonction du feedback des users et recalculer des nouveaux score. Modèle de Rocchio pour les modèles vectoriel \begin{itemize}
    \item Par le retour de l'utilisateur, deux ensembles de vecteur de document : les documents pertinent VS les non pertinents
    \item Vecteur moyen des documents pertinent $\rightarrow$ Correction de notre vecteur de question 
\end{itemize}
Limite : 
\begin{itemize}
    \item Fiabilité des users sur les retours positif négatif 
    \item Comment ils évalue la pertinence
    \item Mais on garde un effet de masse qui moyenne les erreurs 
\end{itemize}

\subsection{Pseudo revelance feedback}
Sugestion d'une nouvelle requete en se basant sur les $ k $ premier document. Pour la trouver méthode de clustering, similarité des termes, ...

Limite : 
\begin{itemize}
    \item Couteux
    \item Query drift : si les top documents ne sont pas pertinents, la requete reformulée ne reflètera jamais le besoin de l'utilisateur (exemple : Apple et apple :  on vas biaiser la requete vers un seul des deux sens)
\end{itemize}

\section{Métrique}
\subsection{Métrique orientées rappel/précision}


\subsection{Métrique orientées rang}

\end{document}