\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}

\graphicspath{{./RI/}}

\title{Fiche RI}
\author{Charles Vin}
\date{2023}

\begin{document}
\maketitle

\section{Généralité}
\begin{itemize}
    \item RI ad-hoc : c'est ce qu'on fait : trouver parmi un ensemble d'articles ceux qui concernent un sujet spécifique.
    \item Indexation = encodage des documents avec un modèle RI
    \item Deux types d'index \begin{itemize}
        \item Index normal : Document : (terme, nombre)
        \item Index inversé : Mot : (document, nombre)
    \end{itemize}
    \item Stemming : ne garde que la racine des mots, un peu moche 
    \item Lematization : retour vers un mot complet
    \item Strategie de recherche : \begin{itemize}
        \item Problème : On a beaucoup de doc, on cherche les K premiers
        \item Deux strat  pour index inversé : \begin{itemize}
            \item TAAT : traiter les terme un par un, fonctionne bien sur des petits corpus où il y a une grande différence de score
            \item DAAT : traiter les doc un par un, plus efficace pour les grandes collections, plus 
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Métrique}
Précision et rappel
\begin{itemize}
    \item Recall : Pourcentage de documents pertinents renvoyés parmi tous ceux qui sont pertinents. Utilisé quand les faux négatifs sont couteux (exemple : le médical).
    \[
        \frac{\left\| R \cap P \right\| }{\left\| P \right\| }
    .\]
    Avec $ R $ l'ensemble des documents renvoyés et $ P $ les documents pertinents. 

    \item Précision : Pourcentable de documents pertinents renvoyés parmis ceux renvoyés. Utilisé quand les faux positif sont couteux (exemple : la RI).
    \[
        \frac{\left\| R \cap P \right\| }{\left\| R \right\| }
    .\]
    Avec $ R $ l'ensemble des documents renvoyés et $ P $ les documents pertinents.

    \item C'est un compromi, augmenter l'un fait baisser l'autre
\end{itemize}
\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=.5\textwidth]{precision_recall_conf_matrix.png}
    \caption{En rouge la précision, en jaune le rappel}
    \label{fig:precision_recall}
\end{figure}

\section{Loi de Zipf}
Stipule que la fréquence d'occurrence d'un mot est inversement proportionnelle à celle de son rang. Le 1er mot est environ 2 fois plus fréquent que le 2nd qui est 2 fois plus fréquent que le 3e etc.. 
\[
    \frac{ \frac{1 }{r^s}}{ \sum_{n=1}^{N} \frac{1}{N}} \approx \frac{1}{r^s}
.\]
Avec $ r $ le rang, $ N $ la taille du corpus et $ s $ un paramètre spécifique au corpus

\section{Loi de Heaps}
Lien entre le nombre de mots distinct et le nombre de mots : \begin{itemize}
    \item les nouveaux mots apparaissent moins fréquenmment quand le vocabulaire croît. 
    \item La taille du vocabulaire n'a pas de borne supérieure (nom propres, erreur de typo)
\end{itemize} 
\[
    V = K n ^\beta 
.\]
Avec $ V $ taille du vocabulaire, $ N $ taille du texte, $ K, \beta  $ paramètre spécifique du texte.

\section{TF-IDF}
\begin{itemize}
    \item Term Frequency : Une pondération locale, nombre occurrences du terme dans le document
    \item Inverse Document frequency : Une pondération globale, fréquence inverse décroit vers 0 si le terme apparait dans tous les documents $ \frac{N}{df(t_i)} $ avec $ df $ nombre de documents contenant le terme. 
    \item TF-IDF : $ tf * idf $, il existe plein de variance
\end{itemize}


\section{Modèle booléen}
\section{Modèle vectoriel}
\section{Modèle probabiliste}
Soit $ R $ une v.a.r binaire pour un dociment $ d $ est pertinent pour la question $ q $. 
\begin{itemize}
    \item On cherche la proba $ P(R | q,d) $ que le document soit pertinent pour la question et le document.
    \item Par bayes on tombe sur $ P(d | R, q) P(R | q ) $.
    \item Un document se décompose en terme \textbf{indépendant} de cette manière  $d : (\bigwedge_{t \in d} ) \wedge (\bigwedge_{t \not \in d} t \not \in d)$
    \item $ \prod_{t \in d}^{} P(t \in d | R, q) \prod_{t \not\in d} P(t \not \in d | R, q) P( R | q) $
    \item Et on peut estimer la proba qu'un terme apparaisse dans un document $ p_t = P(t \in d | R, q), 1 - p_t = P(t \not\in d | R, q) $.
    \item Finalement on peut développer un peu avec un tricks sur le produit et virer les constantes\begin{align*}
        P(R | d,q) &= P(R|q) \prod_{t \in d} p_t \prod_{t \not\in d} 1 - p_t \\
                &= P(R | q) \prod_{t \in d}^{} p_t \frac{1}{1 - p_t} \prod_{t \in \mathcal{T}}^{} 1 - p_t \\ 
                &\propto \prod_{t \in d}^{} p_t \frac{1}{1 - p_t}
    \end{align*}
    \item De base le model pose $ \frac{P(R | q, d)}{P(\not R | q, d)} $, même développement qu'au dessus, puis passage au log pour obtenir un score 
    \[
        s(q,d) = \sum_{t \in q \sqcap d}^{} \log \frac{p_t (1 - u_t)}{u_t (1 - p_t)}
    .\]
\end{itemize}

\subsection{BM25/Okapi}
Si $ t \not\in d \Leftrightarrow TF_t = 0 $ puis avec une modelisation de poisson sur la valeur de ce terme, on retombe sur la formule du coef BM25. Woah qu'est ce que c'est que ce trucs c'est le futur à estimer les param de la poisson

\section{Modèle de langue }
Basé sur l'idée que pour faire une recherche on imagine les mots que le documents pertinent vas contenir. Quel est la proba que le document soit généré par le même modèle de langue que le document.

\end{document}