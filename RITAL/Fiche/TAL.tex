\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}

\graphicspath{{./TAL/}}

\title{Cours}
\author{Charles Vin}
\date{Date}

\begin{document}
\maketitle

\section{Généralité}
\paragraph*{Métrique}
\begin{itemize}
    \item Taux de reconnaissance : $ \frac{bonne pred}{total pred} $ 
    \item Précision pour une classe $ c $: $ \frac{N^C_{correct}}{N^C_{predits}} $ 
    \item Rappel / Recall pour une classe $ c $ : $ \frac{N^C_{correct}}{N^C_{tot}} $
    \item F1 : need plus de vulgarisation
    \item ROC : Faux positif VS Vrais positif
    \item AUC : 
\end{itemize}

\paragraph*{Problème d'équilibre des classes}
\begin{itemize}
    \item Accuracy poubelle $\rightarrow$ Utiliser les autres metrics : AUC / ROC
    \item Ré-équilibrer le jeux de données : supprimer des données dans la classe majoritaire 
    \item Fonction de coût : pénaliser plus les erreurs dans la classe minoritaire (cours 1 diapo 51)
\end{itemize}

\paragraph*{Problème de dimension}
Ajouter un terme sur la fonction coût (ou vraisemblance) pour pénaliser le nombre (ou le poids) des coefficients utilisés pour la décision. (Cours 1 diapo 50)

\paragraph*{TF-IDF encoding} if word k is in most documents, it is probably useless $\rightarrow$ TF-Idf encoding : Donne plus de poids au keyword et un petit peu moins au stopword. A combiner avec blacklist \textbf{EXPLIQUER PLUS LES MATH ?? car ça à l'air important on 'lutilise tout le temps}

\paragraph*{Zipf law}


\section{Liste des pré-processing}
\subsection{Noisy entity removal}
\begin{itemize}
    \item Ponctuation, capital/lower case
    \item Stop word 
    \item Rare word (less than a threshold)
\end{itemize}

\subsection{Text Normalization}
\begin{itemize}
    \item Lemmatization : Lions $\rightarrow$ Lion, are $\rightarrow$ be
    \item Stemming
\end{itemize}

\subsection{Word standardisation}
Regular expression, e.g. for removing "." or expanding words’ contractions ("I’ll" → "I will")


\section{Bag of Word}

\subsection{Stengths and drawbacks}
Avantage : 
\begin{itemize}
    \item Easy light fast 
    \item Opportunity to enrich (Context encoding, Part Of Speech)
    \item Efficient implementation
    \item Still very effective with classification
\end{itemize}
Limite : 
\begin{itemize}
    \item Loose document / sentence structure : mitigated with N-gram
    \item Several task missing : POS tagging, text generation
    \item Semantic gap : On peut pas utiliser la distance euclidienne pour mesurer la différence sémantique
\end{itemize}

\subsection{Classification}
\paragraph*{Naive Bayes}
Rapide, interprétable, naturellement multiclasse. Perf à améliorer. Bien filtrer les stop word. Extention par robustesse (?)

\paragraph*{Classifieur linéaire}
Scalable, attention sensible au dimension

\section{Apprendre la sémantique}
\begin{itemize}
    \item Première approche : WordNet : trop statique (nouvelle expression, hashtag, vocab tecnique) et fait à la main 
    \item Deuxième approche : Mesure de co-occurence : "Fair \textbf{and} legitimate", "Fair \textbf{but} brutal" $\rightarrow$ Marche bien + combo avec BoW
\end{itemize}

\section{Unsupervised approches}
\subsection{LSA : Latent Semantic Analysis}
On un un vocabulaire de taille $ d $, $ N $ document, une LSA avec $ k $ greatest singular values.

On décompose la matrice $ X \in \mathbb{Q}^{N \times d}$ en \begin{itemize}
    \item $ U \in \mathbb{R}^{k \times d} $ : une ligne de $ U $ représente un vecteur de poids par mot. Les poids les plus haut donne la classe du document (?)
    \item $ \Sigma \in \mathbb{R}^{k \times k} $
    \item $ V \in \mathbb{R}^{d \times k} $ Un colonne de $ V $ représente le champs lexical d'une classe (?). C'est les mots les plus important.
    \item Est-ce que $ V $ et $ U $ sont pareil ?? 
    \item Comment on classif un nouveau document la dedans ? 
\end{itemize}

\subsubsection{Stength and drawbacks}
\begin{itemize}
    \item Bon combo avec t-SNE.
    \item Entièrement basé sur BoW : même problème  plus +\begin{itemize}
        \item Not robust to stop word (=high singular values)
        \item Problème forme négative 
    \end{itemize}
    \item Problème avec les mots rare dans les petits corpus (?)
\end{itemize}

\subsection{Kmean}
Comme d'hab, à combo avec LSA pour réduire les dimensions. 
\subsection{Dérivé LSA}
\begin{itemize}
    \item P-LSA : K-mean mais avec un assignement soft mesurer par la probabilité
    \item LDA : On rajoute un prior : modèle plus complexe $\rightarrow$ Les méthodes EM  $\rightarrow$ on utilise MCMC pour estimer les vraissemblance
    \item D'autre extension bien complexe
\end{itemize}





\section{Word Embeddings \& Neural network}
{\large \begin{center}
\textit{Word that appear in similar contexts in text tend to have similar meanings}
\end{center}}

\subsection{Words2Vec}

Implémentation de l'idée précédente : Modèle auto-supervisé, input OH encoding ; + backpropagation 
\begin{itemize}
    \item CBoW : Predict surrounding words (context) from central word ; work well for frequent words
    \item Skipgram : Predict context from central word ; work well for rare words
\end{itemize}
On note : \begin{itemize}
    \item Soft max en sortie sur $ \left| V \right|  $ (taille du vocab) valeur $\rightarrow$ Lourd $\rightarrow$ Evitable par négative sampling je crois ??
    \item Analogie dans l'espace latent $ H $ : Féminin/Masculin, Capitale, sémantique 
    \item \textbf{Difficulté pour aller au delà des mots} : Comment encoder une phrase entière dans $H$ $\rightarrow$ moyenne, sommes, min, max
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics*[width=.75\textwidth]{CBoW.png}
    \caption{CBoW}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics*[width=.75\textwidth]{skipgram.png}
    \caption{Skip-Gram}
\end{figure}

\subsubsection{Glove}
\begin{itemize}
    \item Input : matrice de co-occurence + accès aux word embeding
    \item On veut explicitement que la proximité dans l'espace latent représente la co-occurence des mots
    \item Fonction de cout et de prédiction de GloVe $ \approx  $ PLSA (espace latent pour les co-ocurences) + Contexte local + les words embeding 
\end{itemize}

\subsection{Neural Networks}
\subsubsection{CNN}
How to scale/classify with Word Embeding ? $\rightarrow$ Convolutional Neural networks
\begin{enumerate}
    \item Latent représentation of the word 
    \item Couche de convolution
    \item $\rightarrow$ Une représentation d'un document dans une dimension fixe
    \item $\rightarrow$ Classication : Régréssion, NN w/ soft max, ...
\end{enumerate}
+ une back propagation sur la layer de convolution, l'espace de représentation latent. 

\textbf{Problème} : 2 mois de train 

\subsubsection{RNNs}
\begin{itemize}
    \item Code des séquences \textbf{entière} => parfait pour du texte
    \item Forme de mémoire du passé dans le réseau
    \item Problème de profondeur et d'évanouissement du gradient
\end{itemize}
3 types de classification :
\begin{itemize}
    \item One to many : Image captionning (input une image, sortie plusieurs mots)
    \item Many to one : Sentiment clasification, Question answering (how many ...)
    \item Many to many : Text generation, translation (input anglais sortie français), speech2text 
\end{itemize}
Evolution : 
\begin{itemize}
    \item Architecture spéciale contre la disparition du gradient : LSTM, GRU == long term memory 
    \item Bi-directionnal RNNs : incorporate information from words both preceding and following
\end{itemize}
Toujours problème d'évanouissement du gradient + goulot d'étranglement pour encoder tout une phrase

\subsection{Attention model \& Transformers }
\begin{itemize}
    \item Encode l'information globale (!= CNN que local)
    \item Matrice d'attention == encodage de \begin{itemize}
        \item Chaque mot encodé avec Key, Query, Value
        \item la similarité de chaque mot de la phrase
        \item inclusion du contexte globale
    \end{itemize} 
    \item Multi-head attention : on stack en parallèle les self-attention layer + combinaison
    \item Perte de la position : Fully connected layers: permutation invariant $\rightarrow$ Encodage manuel 
\end{itemize}

\subsubsection{BERT}
Comme word2vec avec de l'attention : pré-train pour apprendre l'espace latent $\rightarrow$ Fine tune sur other task. 

Token de classe CLS $t_0$ qui représente la phrase entière dans un espace latent $\rightarrow$ utile pour prédire la classe avec des modèle classique (regression ect) $\rightarrow$ \textbf{Plus le problème de W2V avec la moyenne ou la somme}

\subsubsection{Transformers Decoder}
GPT
\begin{itemize}
    \item Prédire le mot suivant en fonction du contexte.
    \item Masked Attention: hide (mask) information about future tokens from the model. On donne la phrase au fur et à mesure pour pas tout apprendre d'un coup. On peut pas utiliser les mots d'après vu qu'on génère
\end{itemize}






\end{document}