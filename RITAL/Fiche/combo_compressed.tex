\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=1.25cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}
\usepackage{multicol}
\usepackage{sectsty}
\usepackage{titlesec}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

% \titleformat{\section}[runin]{\footnotesize\bfseries\sffamily}{\thesection}{1em}{}

% \titleformat{\subsection}[runin]{\footnotesize\bfseries\sffamily}{\thesubsection}{1em}{}[]
% \titleformat{\subsubsection}[runin]{\footnotesize\bfseries\sffamily}{\thesubsubsection}{1em}{}[]
% \titleformat{\paragraph}[runin]{\footnotesize\bfseries\sffamily}{\theparagraph}{1em}{}[]
% \titleformat{\subparagraph}[runin]{\footnotesize\bfseries\sffamily}{\thesubparagraph}{1em}{}[]


\titleformat{\section}[runin]{\normalfont\footnotesize\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}[runin]{\normalfont\footnotesize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}[runin]{\normalfont\footnotesize\bfseries}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}[runin]{\normalfont\footnotesize\bfseries}{\theparagraph}{1em}{}
\titleformat{\subparagraph}[runin]{\normalfont\footnotesize\bfseries}{\thesubparagraph}{1em}{}

\titlespacing*{\section}{0pt}{0.8mm}{0.1mm}
\titlespacing*{\subsection}{0pt}{0.1mm}{0.1mm}

\begin{document}
\footnotesize
\begin{multicols}{2}

    \section{Généralité}
    Voir la fiche RI pour : métrique, zipf law, tf-idf

    Le fossé sémantique, ou "semantic gap", désigne l'écart entre les représentations de bas niveau des données (par exemple, les caractéristiques syntaxiques ou les mots individuels) et les représentations de haut niveau (par exemple, la signification ou le contexte) qui sont plus proches de la compréhension humaine. 

    \paragraph*{Problème d'équilibre des classes}
    \begin{itemize}
        \item Accuracy poubelle $\rightarrow$ Utiliser les autres metrics : AUC / ROC
        \item Ré-équilibrer le jeux de données : supprimer des données dans la classe majoritaire 
        \item Fonction de coût : pénaliser plus les erreurs dans la classe minoritaire (cours 1 diapo 51)
    \end{itemize}

    \paragraph*{Problème de dimension}
    Ajouter un terme sur la fonction coût (ou vraisemblance) pour pénaliser le nombre (ou le poids) des coefficients utilisés pour la décision. (Cours 1 diapo 50)

    \paragraph*{TF-IDF encoding} if word k is in most documents, it is probably useless $\rightarrow$ TF-Idf encoding : Donne plus de poids au keyword et un petit peu moins au stopword. A combiner avec blacklist. Plus d'info dans la fiche RI
    
    \section{Liste des pré-processing}
    \subsection{Noisy entity removal}
    \begin{itemize}
        \item Ponctuation, capital/lower case
        \item Stop word 
        \item Rare word (less than a threshold)
    \end{itemize}
    
    \subsection{Text Normalization}
    \begin{itemize}
        \item Lemmatization : Lions $\rightarrow$ Lion, are $\rightarrow$ be
        \item Stemming
    \end{itemize}
    
    \subsection{Word standardisation}
    Regular expression, e.g. for removing "." or expanding words’ contractions ("I’ll" → "I will")
    
    
    \section{Bag of Word}
    
    \subsection{Stengths and drawbacks}
    Avantage : 
    \begin{itemize}
        \item Easy light fast 
        \item Opportunity to enrich (Context encoding, Part Of Speech)
        \item Efficient implementation
        \item Still very effective with classification
        \item Simple à mettre en œuvre, facile à interpréter, conserve la fréquence des mots.
    \end{itemize}
    Limite : 
    \begin{itemize}
        \item Ne capture pas l'ordre des mots, génère des vecteurs de grande taille, peut être sensible au bruit.
        \item Loose document / sentence structure : mitigated with N-gram
        \item Several task missing : POS tagging, text generation
        \item Semantic gap : On peut pas utiliser la distance euclidienne pour mesurer la différence sémantique
    \end{itemize}
    Représentation en tri-grammes de lettres :\\
    Avantages : Capture les motifs locaux et les sous-chaînes fréquentes, réduit la taille du vocabulaire, résilient aux fautes d'orthographe. capturer des motifs spécifiques à un auteur.\\
    Inconvénients : Peut générer du bruit, perd la signification des mots complets, moins interprétable.
    
    \subsection{Classification}
    \paragraph*{Naive Bayes}
    Rapide, interprétable, naturellement multiclasse. Perf à améliorer. Bien filtrer les stop word. Extention par robustesse (?)
    
    \paragraph*{Classifieur linéaire}
    Scalable, attention sensible au dimension
    
    \section{Apprendre la sémantique}
    \begin{itemize}
        \item Première approche : WordNet : trop statique (nouvelle expression, hashtag, vocab tecnique) et fait à la main 
        \item Deuxième approche : Mesure de co-occurence : "Fair \textbf{and} legitimate", "Fair \textbf{but} brutal" $\rightarrow$ Marche bien + combo avec BoW
    \end{itemize}
    
    \section{Unsupervised approches}
    \subsection{LSA : Latent Semantic Analysis}
    But : réduire la dimension, réduidre le bruit. Hyperparamètres : Nombre de sujets, hyperparamètres de la distribution de Dirichlet
    On un un vocabulaire de taille $ d $, $ N $ document, une LSA avec $ k $ plus grande valeur propre, qui forme $ k $ grand "concept" du corpus.
    
    On décompose la matrice des occurence $ X \in \mathbb{Q}^{N \times d}$ en \begin{itemize}
        \item $ U \in \mathbb{R}^{k \times d} $ : une ligne de $ U $ représente un vecteur de poids par mot dans chacun des $ k $ concept.
        \item $ \Sigma \in \mathbb{R}^{k \times k} $ valeur propre $\rightarrow$ \textbf{Tri par ordre croissant}pour garder les $k$ plus grande valeur (comme en BIMA)
        \item $ V \in \mathbb{R}^{d \times k} $ Un colonne de $ V $ représente l'intensité des relations entre le document $j$ et chaque concept
        \item Est-ce que $ V $ et $ U $ sont pareil ?? 
        \item Comment on classif un nouveau document la dedans ? Soit $ q $ un nouveau document ou une query, $ \hat{q} = \sigma ^{-1} U^{-1} q $ 
    \end{itemize}
    Un ligne de la matrice de la matrice d'ocurence $ t_i $, une colonne $ d_i $. $ t_i^T t_p $ corrélation de ces deux terme sur tout le corpus $\rightarrow X X^t$. $ d_j^T d_q $ corrélation entre ces deux document pour tout terme $\rightarrow X^T X$
    
    \subsubsection{Stength and drawbacks}
    \begin{itemize}
        \item Bon combo avec t-SNE.
        \item Entièrement basé sur BoW : même problème  plus +\begin{itemize}
            \item Not robust to stop word (=high singular values)
            \item Problème forme négative 
        \end{itemize}
        \item Problème avec les mots rare dans les petits corpus (?)
        \item Peu combiner des dimension qui non pas de lien entre elle (combiner linéairement voiture et camion ok mais pas voiture et bouteille)
        \item Mauvaise prise en compte des doubles sens des mots, car un mot = un point de l'espace sémantique
    \end{itemize}
    
    \subsection{Kmean}
    Comme d'hab, à combo avec LSA pour réduire les dimensions. 
    \subsection{Dérivé LSA}
    \begin{itemize}
        \item P-LSA : K-mean mais avec un assignement soft mesurer par la probabilité
        \item LDA : On rajoute un prior : modèle plus complexe $\rightarrow$ Les méthodes EM  $\rightarrow$ on utilise MCMC pour estimer les vraissemblance
        \item D'autre extension bien complexe
    \end{itemize}
    
    
    
    
    
    \section{Word Embeddings \& Neural network}
    {\large \begin{center}
    \textit{Word that appear in similar contexts in text tend to have similar meanings}
    \end{center}}
    
    \subsection{Words2Vec}
    Apprendre des représentations vectorielles des mots (word embeddings) en exploitant les relations contextuelles entre les mots.
    Implémentation de l'idée précédente : Modèle auto-supervisé, input OH encoding ; + backpropagation 
    \begin{itemize}
        \item Réseau de neurone peu profond pour :
        \item CBoW : Predict surrounding words (context) from central word ; work well for frequent words
        \item Skipgram : Predict context from central word ; work well for rare words
    \end{itemize}
    On note : \begin{itemize}
        \item Soft max en sortie sur $ \left| V \right|  $ (taille du vocab) valeur $\rightarrow$ Lourd $\rightarrow$ Evitable par négative sampling je crois ??
        \item Analogie dans l'espace latent $ H $ : Féminin/Masculin, Capitale, sémantique 
        \item \textbf{Difficulté pour aller au delà des mots} : Comment encoder une phrase entière dans $H$ $\rightarrow$ moyenne, sommes, min, max
        \item Hyperparamètres : Taille du vecteur, fenêtre contextuelle, nombre d'itérations, taux d'apprentissage.
    \end{itemize}
    
    \subsubsection{Glove}
    \begin{itemize}
        \item Input : matrice de co-occurence + accès aux word embeding
        \item On veut explicitement que la proximité dans l'espace latent représente la co-occurence des mots
        \item Fonction de cout et de prédiction de GloVe $ \approx  $ PLSA (espace latent pour les co-ocurences) + Contexte local + les words embeding 
    \end{itemize}
    
    \subsection{Neural Networks}
    \subsubsection{CNN}
    How to scale/classify with Word Embeding ? $\rightarrow$ Convolutional Neural networks
    \begin{enumerate}
        \item Latent représentation of the word 
        \item Couche de convolution
        \item $\rightarrow$ Une représentation d'un document dans une dimension fixe
        \item $\rightarrow$ Classication : Régréssion, NN w/ soft max, ...
    \end{enumerate}
    + une back propagation sur la layer de convolution, l'espace de représentation latent. 
    
    \textbf{Problème} : 2 mois de train 
    
    \subsubsection{RNNs}
    \begin{itemize}
        \item Code des séquences \textbf{entière} => parfait pour du texte
        \item Forme de mémoire du passé dans le réseau
        \item Problème de profondeur et d'évanouissement du gradient
    \end{itemize}
    3 types de classification :
    \begin{itemize}
        \item One to many : Image captionning (input une image, sortie plusieurs mots)
        \item Many to one : Sentiment clasification, Question answering (how many ...)
        \item Many to many : Text generation, translation (input anglais sortie français), speech2text 
    \end{itemize}
    Evolution : 
    \begin{itemize}
        \item Architecture spéciale contre la disparition du gradient : LSTM, GRU == long term memory 
        \item Bi-directionnal RNNs : incorporate information from words both preceding and following
    \end{itemize}
    Toujours problème d'évanouissement du gradient + goulot d'étranglement pour encoder tout une phrase
    
    \subsection{Attention model \& Transformers }
    \begin{itemize}
        \item Encode l'information globale (!= CNN que local)
        \item Matrice d'attention == encodage de \begin{itemize}
            \item Chaque mot encodé avec Key, Query, Value
            \item la similarité de chaque mot de la phrase
            \item inclusion du contexte globale
        \end{itemize} 
        \item Multi-head attention : on stack en parallèle les self-attention layer + combinaison
        \item Perte de la position : Fully connected layers: permutation invariant $\rightarrow$ Encodage manuel 
    \end{itemize}
    
    \subsubsection{BERT}
    Comme word2vec avec de l'attention : pré-train pour apprendre l'espace latent $\rightarrow$ Fine tune sur other task. 
    
    Token de classe CLS $t_0$ qui représente la phrase entière dans un espace latent $\rightarrow$ utile pour prédire la classe avec des modèle classique (regression ect) $\rightarrow$ \textbf{Plus le problème de W2V avec la moyenne ou la somme}
    
    \subsubsection{Transformers Decoder}
    GPT
    \begin{itemize}
        \item Prédire le mot suivant en fonction du contexte.
        \item Masked Attention: hide (mask) information about future tokens from the model. On donne la phrase au fur et à mesure pour pas tout apprendre d'un coup. On peut pas utiliser les mots d'après vu qu'on génère
    \end{itemize}













\section{Généralité}
\begin{itemize}
    \item RI ad-hoc : c'est ce qu'on fait : trouver parmi un ensemble d'articles ceux qui concernent un sujet spécifique.
    \item Indexation = encodage des documents avec un modèle RI
    \item Deux types d'index \begin{itemize}
        \item Index normal : Document : (terme, nombre/score)
        \item Index inversé : Mot : (document, nombre/score)
    \end{itemize}
    \item Requete sur index : on somme les scores pour obtenir le score final d'un document
    \item Stemming : ne garde que la racine des mots, un peu moche 
    \item Lematization : retour vers un mot complet
    \item Strategie de recherche : \begin{itemize}
        \item Problème : On a beaucoup de doc, on cherche les K premiers
        \item Deux strat  pour index inversé : \begin{itemize}
            \item TAAT : traiter les terme un par un, fonctionne bien sur des petits corpus où il y a une grande différence de score \begin{enumerate}
                \item Trier l'index de chaque terme par score croissant -> une grosse liste ordonnée terme puis score 
                \item Parcourir par terme et maintenir un dictionnaire avec les scores par document, optimisation : utilisation d'une Heap
            \end{enumerate}
            \item DAAT : traiter les doc un par un, plus efficace pour les grandes collections, plus 
        \end{itemize}
    \end{itemize}
\end{itemize}



\section{Loi de Zipf}
Stipule que la fréquence d'occurrence d'un mot est inversement proportionnelle à celle de son rang dans la liste des mots classés par fréquence. Les mot les plus fréquent sont beaucoups plus fréquents que moins fréquent. Le 1er mot est environ 2 fois plus fréquent que le 2nd qui est 2 fois plus fréquent que le 3e etc.. 
\[
    \frac{ \frac{1 }{r^s}}{ \sum_{n=1}^{N} \frac{1}{N}} \approx \frac{1}{r^s}, frequence = \frac{\lambda }{rank}
.\]
Avec $ r $ le rang, $ N $ la taille du corpus et $ s $ un paramètre spécifique au corpus.

\section{Loi de Heaps}
Lien entre le nombre de mots distinct et le nombre de mots : \begin{itemize}
    \item les nouveaux mots apparaissent moins fréquenmment quand le vocabulaire croît. 
    \item La taille du vocabulaire n'a pas de borne supérieure (nom propres, erreur de typo)
\end{itemize} 
\[
    V = K n ^\beta 
.\]
Avec $ V $ taille du vocabulaire, $ N $ taille du texte, $ K, \beta  $ paramètre spécifique du texte.

\section{TF-IDF}
\begin{itemize}
    \item Term Frequency : Une pondération locale, fréquence du terme dans le document $ \frac{f_{t,d}}{\sum_{t' \in d}^{} f_{t', d}} $ 
    \item Inverse Document frequency : Une pondération globale, fréquence inverse décroit vers 0 si le terme apparait dans tous les documents $ \log_{} \frac{N}{df(t_i)} $ avec $ df $ nombre de documents contenant le terme, $ N $ nombre de document. 
    \item TF-IDF : $ tf * idf $, il existe plein de variance. En particulier en TD on a fait un "count idf" : $ f_{t,d} \log_{} \frac{N}{n_t}, n_t = $ nombre de document où $ t $ est présent.
\end{itemize}


\section{Modèle booléen}
formule logique into score binaire, pas de pondération rien
\section{Modèle vectoriel}
Score de distance entre deux vecteurs : un de document score (classiquement un tf) et celui de la requète (classiquement binaire par terme). Inner product $ <X,Y> $, cosinus $ \frac{<X,Y>}{\left\| X \right\| \left\| Y \right\| } $ 

\section{Modèle probabiliste}
Soit $ R $ une v.a.r binaire pour un dociment $ d $ est pertinent pour la question $ q $. 
\begin{itemize}
    \item On cherche la proba $ P(R | q,d) $ que le document soit pertinent pour la question et le document.
    \item Par bayes on tombe sur $ P(d | R, q) P(R | q ) $.
    \item Un document se décompose en terme \textbf{indépendant} de cette manière  $d : (\bigwedge_{t \in d} ) \wedge (\bigwedge_{t \not \in d} t \not \in d)$
    \item $ \prod_{t \in d}^{} P(t \in d | R, q) \prod_{t \not\in d} P(t \not \in d | R, q) P( R | q) $
    \item Et on peut estimer la proba qu'un terme apparaisse dans un document pertinent $ p_t = P(t \in d | R, q), 1 - p_t = P(t \not\in d | R, q) $. 
    \item Finalement on peut développer un peu avec un tricks sur le produit et virer les constantes\begin{align*}
        P(R | d,q) &= P(R|q) \prod_{t \in d} p_t \prod_{t \not\in d} 1 - p_t \\
                &= P(R | q) \prod_{t \in d}^{} p_t \frac{1}{1 - p_t} \prod_{t \in \mathcal{T}}^{} 1 - p_t \\ 
                &\propto \prod_{t \in d}^{} p_t \frac{1}{1 - p_t}
    \end{align*}
    \item De base le model pose un rapport de vraissemblance pour avoir un score $ \frac{P(R | q, d)}{P(\not R | q, d)} $, même développement qu'au dessus, puis passage au log. 
    \[
        s(q,d) = \sum_{t \in q \sqcap d}^{} \log \frac{p_t (1 - u_t)}{u_t (1 - p_t)}
    .\]
    En pratique : $ s(q,d) = \sum_{t \in q \sqcap d}^{} \log_{} \frac{a + 0.5}{c + O.5} \frac{d + 0.5}{b+0.5} $ avec $a=$nb appartition terme dans doc pertinent, $b$ nb apparition terme dans document non pertinent, $c$ nb de \textbf{non} apparitition dans document pertinent et $d$ nb \textbf{non} apparition terme dans document non pertinenet
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|}
        \hline
            ~ & Doc Pertinent & Non Pertinent \\ \hline
            $t \in d$ & a & b \\ \hline
            $t \not\in d$ & c & d \\ \hline
        \end{tabular}
    \end{table}
    \item On estime les proba par max vraissemblance qui donne juste la fréquence des termes
    \item On peut intégré un prior sur $ P(d) $ mais je sais honnetement pas à quelle étape : longueur du doc, longueur moyenne des mots, date, nombre de lien, pagerank
\end{itemize}

\subsection{BM25/Okapi}
Si $ t \not\in d \Leftrightarrow TF_t = 0 $ puis avec une modelisation de poisson sur la valeur de ce terme, on retombe sur la formule du coef BM25. Woah qu'est ce que c'est que ce trucs c'est le futur à estimer les param de la poisson

BM25 est un modèle de sac de mots qui ordonne les documents en fonction de la fréquence des termes qui apparaissent dans chaque document, indépendamment des relations pouvant exister entre ces termes ou de leurs proximités relatives au sein du document. Pour une requête $ Q $ contenant les mots $ q_1, \dots, q_n $, le score BM25 d'un document $ D $ est $ s(D,Q) = \sum_{i=1}^{n}IDF(q_i) \frac{f(q_i, D) (k_1 + 1)}{f(q_i, D) + k_1 (1 - b + b \frac{\left| D \right| }{avgdl})} $ avec $ f(q_i, D) = tf_i $  fréquence du terme $ q_i $ dans le document $ D $, $ \left| D \right|  $ Longueur du doc $ D $ (nombre de mot), $ avgdl $ longueur moyenne des documents de la collection, $ k_1 = 1.2, b = 0.75 $, $ IDF(q_i) = \log_{} \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} $ avec $ n(q_i) $ le nombre de document contenant $ q_i $ 
\section{Modèle de langue }
Basé sur l'idée que pour faire une recherche on imagine les mots que le documents pertinent vas contenir. Quel est la proba que le document soit généré par le même modèle de langue que le document.

\[
    p(t_1, \dots, t_n) = \sum_{t}^{}tf(t)\log_{} P(t | \theta _{Md}) = \sum_{t}^{}tf(t)\log_{} p_t
.\]
Par max vraissemblance + langragien  $ p_t = \frac{tf_(t)}{\sum_{t}^{}tf(t)} $ . C'est très très flou dans le diapo.

Dans le cas où un mot de la requête n'apparrait pas dans le document, score = 0 $\rightarrow$ Lissage des probas = modèle de
mélange multinomial entre la distribution des termes dans le
document et la distribution des termes dans la collection = Dirichlet ou Jelinek-Mercer

\section{Reformulation de requete}
\subsection{Revelance feedback}
Recalculer les poids des document en fonction du feedback des users et recalculer des nouveaux score. Modèle de Rocchio pour les modèles vectoriel \begin{itemize}
    \item Par le retour de l'utilisateur, deux ensembles de vecteur de document : les documents pertinent VS les non pertinents
    \item Vecteur moyen des documents pertinent $\rightarrow$ Correction de notre vecteur de question 
    \[
        q_{new} = a q_{old} + b * d^+ - c d^-
    .\]
    avec $ d^+ $ le vecteur moyen des documents pertinents, same pour $ d^- $. Puis binariser q avec un seuil
\end{itemize}
Limite : 
\begin{itemize}
    \item Fiabilité des users sur les retours positif négatif 
    \item Comment ils évalue la pertinence
    \item Mais on garde un effet de masse qui moyenne les erreurs 
\end{itemize}

\subsection{Pseudo revelance feedback}
Sugestion d'une nouvelle requete en se basant sur les $ k $ premier document. Pour la trouver méthode de clustering, similarité des termes, ...

Limite : 
\begin{itemize}
    \item Couteux
    \item Query drift : si les top documents ne sont pas pertinents, la requete reformulée ne reflètera jamais le besoin de l'utilisateur (exemple : Apple et apple :  on vas biaiser la requete vers un seul des deux sens)
\end{itemize}

\section{Métrique}
\subsection{Métrique orientées rappel/précision}
Précision et rappel
\begin{itemize}
    \item Recall : Pourcentage de documents pertinents renvoyés parmi tous ceux qui sont pertinents. Utilisé quand les faux négatifs sont couteux (exemple : le médical, documentaliste).
    \[
        \frac{\left\| R \cap P \right\| }{\left\| P \right\| }
    .\]
    Avec $ R $ l'ensemble des documents renvoyés et $ P $ les documents pertinents. 

    \item Précision : Pourcentable de documents pertinents renvoyés parmis ceux renvoyés. Utilisé quand les faux positif sont couteux (exemple : la RI : moteur de recherche).
    \[
        \frac{\left\| R \cap P \right\| }{\left\| R \right\| }
    .\]
    Avec $ R $ l'ensemble des documents renvoyés et $ P $ les documents pertinents.

    \item C'est un compromi, augmenter l'un fait baisser l'autre
    \item F-mesure 
    \[
        F_\beta = (1 + \beta ^2) \frac{P * R}{\beta ^2 (P+R)}
    .\]

    \item Tracer une courbe précision recall : \begin{enumerate}
        \item faire un tableau avec pour colonne : rang, (probabilité), y true, recall, precision.
        \item Calculer le recal et la précision pour chaque ligne en rajoutant les résultats d'avant
        \item Puis pour tracer la courbe : chaque ligne = un point du graph
    \end{enumerate}
    
    \item Précision interpolée : super visuel, pour tracer la courbe, on fait varier le recall, pour chaque point on regarde la précision max à droite de ce point. Soit $ r $  un point de recall, $ P_{interp}(r) = \max _{r' \geq r} P(r') $ 
    
    \item Précision moyenne : moyenne arithmétique de la précision interpolé \\
    OU $ \approx \text{AveP} = \frac{\sum_{k=1}^n P(k) \times {1}_{doc_k \text{ is revelant}}} {\text{total number of relevant documents}}$ 
    \item \textbf{Moyenne des précision moyenne (MAP)} = moyenne des précision moyenne pour chaque requette
    
\end{itemize}

\subsection{Métrique orientées rang}
\begin{itemize}
    \item \textbf{Moyenne des rangs inverse (MRR)} : moyenne de l'inverse du rang du premier document pertinent renvoyé sur l'ensemble des requêtes $ \frac{1}{\left| Q \right| } \sum_{q \in Q}^{} \frac{1}{rank_i} $ where $ \text{rank}_{i}$ refers to the rank position of the first relevant document for the i-th query.
    
    \item \textbf{Discounted cumulative gain (DCG)} :  Somme pondéré par $ \frac{1}{\log_{2} (rang)} $ du score de pertinence des documents. $ DCG_p = score_1 + \sum_{i=1}^{p} \frac{score_i}{\log_{2} i} $ pour une requète. 
    \item Normlized DCG : pour pouvoir faire une moyenne sur l'ensemble des requète il faut normaliser le DCG en utilisant le IDCG (liste idéale de résultat) $ nDCG_p = \frac{DCG_p}{IDCG_p} $ 
\end{itemize}

\section{Apprentissage de model}
\begin{itemize}
    \item Distillation : Faire apprendre un premier modèle moins gourmand en ressource, qui fera beaucoup de faux négatif, puis entrainer un autre modèle par dessus ces prédictions $\rightarrow$ plus robuste au bruit car il est pas directement exposé au faux negatif. Très efficace
\end{itemize}
\subsection{Modèle dense / de représentation}
\begin{itemize}
    \item Représenter la query et l'input dans un espace latent (le même ou non) de même taille puis score de similarité (cos ou autre)
    \item \textbf{Gros overfit} car augmenter le score $ \Leftrightarrow $ augmenter la norme et espace assez grand pour overfit chaque doc
    \item \textbf{forward lent}
    \item Technique de clusterisation : \begin{itemize}
        \item Pour eviter l'overfit pendant l'apprentisage batch uniquement du même cluster
        \item Pour faire moins de produit scalaire car couteux, produit scalaire avec représentatn des clusters
    \end{itemize}
\end{itemize}

\subsection{Modèle d'intération}
\begin{itemize}
    \item Intération faible : matrice pour faire la similarité into NN score (marchait pas beaucoup)
    \item Intéraction forte : concaténation de la query et du doc into NN score (cross encoder)
    \item Cross Encoder : Transformer + input concat query doc $\rightarrow$ Classifier linéaire sur la sortie en grande dimension $\rightarrow$ score ; gourmand mais performant 
    \item Mécanisme en deux temps : gros tri avec un pré-traitement, et ordonancement avec le modèle d'interaction forte
\end{itemize}
\subsection{Modèle sparce}
\begin{itemize}
    \item Rien compris
    \item Doc2query : Doc $\rightarrow$ Question sur le doc $\rightarrow$ concaténation de la question et du doc $\rightarrow$ amélioration des performances de recherche !
    \item CCL:  Bon résultat, mieux que BM25, robuste ; un peu lourd à apprendre ; pour arriver à l'état de l'art il faut quand même ajouter un cross encoder en 2ème étape
\end{itemize}

\section{Page Rank}
\begin{itemize}
    \item Analyse les liens entre les pages, graph orienté de page. une page a un PageRank d'autant plus important qu'est grande la somme des PageRanks des pages qui pointent vers elle
    \item On veut trouver la distribution stationnaire $ s $ du graph (comme dans markov chain) représentant l'importance de chaque page.
    \item $ A $ matrice d'ajascence avec $ a_{ij} = 1$ si lien de $ i $ ver $ j $, $ d_i = \sum_{j}^{} a_{ij} $ nombre de lien entrant pour une page, $ P $ matrice de transition avec $ p_{ij} = \frac{a_{ij}}{d_i}$ proba de transition de $ j $ à $ i $ 
    \item $ s_j = d \sum_{i}^{}p_{ij} s_i + (1 - d) a_j, d$ facteur d'armotissement ($ \approx 0.8 $ )
    \item $ s = dsP + (1-d)a  = s (d P + (1 - d) \mathbf{1}) = s \hat{M}$ 
    \item sur wikipedia c'est toujours $ s = dsP + (1-d)* \frac{1}{N} $, $ N $ nombre de doc.
    \item Version itérative : \begin{enumerate}
        \item Initialisation de $ S = \frac{1}{N} \mathbf{1} $ proba uniforme
        \item Calculer $ \hat{M} = dP + \frac{1-d}{N} \mathbf{1} $ 
        \item $ S_{n+1} = S_n * \hat{M} $ ou $ S_{n+1} = d SP + \frac{1- d}{N} $  
    \end{enumerate}
\end{itemize}


\end{multicols}

\end{document}