{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0S5SZGAZ9wu"
      },
      "source": [
        "# Reconnaissance d'entités nommées avec un Bi-LSTM (pytorch)\n",
        "\n",
        "Avant toute chose, n'oubliez pas de choisir un environnement GPU dans Colab (`Exécution` $\\rightarrow$ `Modifier le type d'exécution`)\n",
        "\n",
        "Xavier Tannier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxo1tPbpfJqq"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Colab & Drive libraries \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogleapiclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhttp\u001b[39;00m \u001b[39mimport\u001b[39;00m MediaIoBaseDownload\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# Colab & Drive libraries \n",
        "from google.colab import files\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import drive\n",
        "# Mount Google drive. This will prompt for authorization.\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCEFbpwmUIB2"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbmA5IXdHX0D"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install torchmetrics --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiqVuZorZI7Y"
      },
      "outputs": [],
      "source": [
        "from os.path import isfile, isdir, join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "import codecs \n",
        "\n",
        "# Manual seed to ensure reproducibility\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43M5y0szhaVs"
      },
      "outputs": [],
      "source": [
        "# Path to dataset\n",
        "train_file = '/content/drive/My Drive/data/conll/eng/train.txt'\n",
        "val_file = '/content/drive/My Drive/data/conll/eng/valid.txt'\n",
        "test_file = '/content/drive/My Drive/data/conll/eng/test.txt'\n",
        "\n",
        "# minimum frequency for a word to have its own embeddings\n",
        "min_word_freq = 2\n",
        "# Batch size\n",
        "batch_size = 64\n",
        "\n",
        "# how big is each word vector (if not preloaded)\n",
        "embed_size = 50 \n",
        "\n",
        "# how many times to iterate over all samples\n",
        "n_epochs = 15 \n",
        "\n",
        "# CPU workers\n",
        "workers = 1\n",
        "\n",
        "# sanity check\n",
        "assert isfile(train_file)\n",
        "assert isfile(val_file)\n",
        "assert isfile(test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZK79zK3oyBX"
      },
      "outputs": [],
      "source": [
        "def read_words_tags(file, tag_ind, caseless=False):\n",
        "    \"\"\"\n",
        "    Reads raw data in the CoNLL 2003 format and returns word and tag sequences.\n",
        "    :param file: file with raw data in the CoNLL 2003 format\n",
        "    :param tag_ind: column index of tag\n",
        "    :param caseless: lowercase words?\n",
        "    :return: word, tag sequences\n",
        "    \"\"\"\n",
        "    with codecs.open(file, 'r', 'utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    words = []\n",
        "    tags = []\n",
        "    temp_w = []\n",
        "    temp_t = []\n",
        "    for line in lines:\n",
        "        if not (line.isspace() or (len(line) > 10 and line[0:10] == '-DOCSTART-')):\n",
        "            feats = line.rstrip('\\n').split()\n",
        "            temp_w.append(feats[0].lower() if caseless else feats[0])\n",
        "            temp_t.append(feats[tag_ind])\n",
        "        elif len(temp_w) > 0:\n",
        "            assert len(temp_w) == len(temp_t)\n",
        "            words.append(temp_w)\n",
        "            tags.append(temp_t)\n",
        "            temp_w = []\n",
        "            temp_t = []\n",
        "    # last sentence\n",
        "    if len(temp_w) > 0:\n",
        "        assert len(temp_w) == len(temp_t)\n",
        "        words.append(temp_w)\n",
        "        tags.append(temp_t)\n",
        "\n",
        "    # Sanity check\n",
        "    assert len(words) == len(tags)\n",
        "\n",
        "    return words, tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggNgwhESAvJj"
      },
      "outputs": [],
      "source": [
        "class NERDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, word_inputs, tag_inputs, sent_lengths, masks):\n",
        "        \"\"\"\n",
        "        :param word_inputs: padded word sequences\n",
        "        :param tag_inputs: padded tag sequences \n",
        "        :param sent_lengths: word sequence lengths\n",
        "        :param masks: masks\n",
        "        \"\"\"\n",
        "        self.word_inputs = word_inputs\n",
        "        self.tag_inputs = tag_inputs\n",
        "        self.sent_lengths = sent_lengths\n",
        "        self.masks = masks\n",
        "\n",
        "        self.data_size = len(self.word_inputs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.word_inputs[i], self.tag_inputs[i], \\\n",
        "               self.sent_lengths[i], self.masks[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQDwibT--niB"
      },
      "outputs": [],
      "source": [
        "def create_conll_dataloader(conll_file, train=False,\n",
        "                            word_map=None, tag_map=None,\n",
        "                            min_word_freq=1, debug=True):\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoader\n",
        "    :param conll_file\n",
        "    :param train: True if the dataset is for training purpose\n",
        "              dataset not for training purpose do not participate to vocabulary\n",
        "              definition\n",
        "    :param word_map: word-to-id mapping (not None for non-training dataset)\n",
        "    :param tag_map: tag-to-id mapping (not None for non-training dataset)\n",
        "    :min_word_freq\n",
        "    :debug: True for a small debugging dataset\n",
        "    \"\"\"\n",
        "    assert train or (word_map is not None and tag_map is not None)\n",
        "    # Read dataset file\n",
        "    tokens, tags = read_words_tags(conll_file, -1)\n",
        "    if debug:\n",
        "        tokens = tokens[:150]\n",
        "        tags = tags[:150]\n",
        "        min_word_freq = 0\n",
        "\n",
        "    # Build vocabulary\n",
        "    if train:\n",
        "        word_freq = Counter()\n",
        "        tag_map = set()\n",
        "\n",
        "        for sentence_index in range(len(tokens)):\n",
        "            sentence_tokens = tokens[sentence_index]\n",
        "            sentence_tags = tags[sentence_index]\n",
        "            word_freq.update(sentence_tokens)\n",
        "            tag_map.update(sentence_tags)\n",
        "\n",
        "        word_map = {k: v + 1 for v, k in enumerate([w for w in word_freq.keys() if word_freq[w] > min_word_freq])}\n",
        "        tag_map = {k: v for v, k in enumerate(tag_map)}\n",
        "        word_map['<pad>'] = 0\n",
        "        #word_map['<end>'] = len(word_map)\n",
        "        word_map['<unk>'] = len(word_map)\n",
        "        #tag_map['<pad>'] = 0\n",
        "        #tag_map['<start>'] = len(tag_map)\n",
        "        #tag_map['<end>'] = len(tag_map)\n",
        "\n",
        "\n",
        "    # Encode sentences into word maps with <end> at the end\n",
        "    word_inputs = list(map(lambda s: torch.LongTensor(list(map(lambda w: word_map.get(w, word_map['<unk>']), s))), tokens))\n",
        "    # Encode tags into tag maps with <end> at the end\n",
        "    tag_inputs = list(map(lambda s: torch.LongTensor(list(map(lambda t: tag_map[t], s))), tags))\n",
        "\n",
        "    # Sentence lengths & masks\n",
        "    sentence_lengths = [len(seq) for seq in word_inputs]\n",
        "\n",
        "    sentence_masks = [torch.BoolTensor([1] * length) for length in sentence_lengths]\n",
        "\n",
        "    # Pad\n",
        "    pad_word_inputs = torch.nn.utils.rnn.pad_sequence(word_inputs, batch_first=True, padding_value=word_map['<pad>'])\n",
        "    pad_tag_inputs = torch.nn.utils.rnn.pad_sequence(tag_inputs, batch_first=True, padding_value=0)\n",
        "    pad_masks = torch.nn.utils.rnn.pad_sequence(sentence_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    assert len(pad_word_inputs) == len(pad_tag_inputs) == len(sentence_lengths) == len(pad_masks)\n",
        "\n",
        "    # Create DataLoader\n",
        "    data_loader = torch.utils.data.DataLoader(NERDataset(pad_word_inputs, pad_tag_inputs, sentence_lengths, pad_masks), \n",
        "                                            batch_size=batch_size, shuffle=train,\n",
        "                                            num_workers=workers, pin_memory=False)\n",
        "    return data_loader, word_map, tag_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnOtkpFICY2c"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Sequence classification module\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, label_number,\n",
        "                 batch_size,\n",
        "                 hidden_size=100, dropout=0.5):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.automatic_optimization = True\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=False, \n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classif = nn.Linear(hidden_size, label_number)\n",
        "\n",
        "        self.loss_fn= nn.NLLLoss(reduction='mean')\n",
        "\n",
        "        self.train_metrics = torchmetrics.MetricCollection({\n",
        "            'precision/train': torchmetrics.Precision(num_classes=label_number, average='macro'),\n",
        "            'recall/train': torchmetrics.Recall(num_classes=label_number, average='macro'),\n",
        "            'F1/train': torchmetrics.F1(num_classes=label_number, average='macro'),\n",
        "            'accuracy/train': torchmetrics.Accuracy()\n",
        "        })\n",
        "        self.val_metrics = torchmetrics.MetricCollection({\n",
        "            'precision/val': torchmetrics.Precision(num_classes=label_number, average='macro'),\n",
        "            'recall/val': torchmetrics.Recall(num_classes=label_number, average='macro'),\n",
        "            'F1/val': torchmetrics.F1(num_classes=label_number, average='macro'),\n",
        "            'accuracy/val': torchmetrics.Accuracy()\n",
        "        })\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = torch.nn.utils.rnn.pack_padded_sequence(h_embedding,\n",
        "                                                                lengths.cpu().numpy(),\n",
        "                                                                batch_first=True,\n",
        "                                                               enforce_sorted=False)\n",
        "        hidden = None\n",
        "        h_lstm, hidden = self.lstm(h_embedding, hidden)\n",
        "        output, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(h_lstm, batch_first=True)  \n",
        "        conc = output\n",
        "        conc = self.dropout(conc)\n",
        "        out = self.classif(conc)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y, lengths, masks = batch\n",
        "        out = self(x, lengths)\n",
        "\n",
        "        pack_masks = torch.nn.utils.rnn.pack_padded_sequence(masks,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        masks, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_masks, batch_first=True)  \n",
        "        pack_y = torch.nn.utils.rnn.pack_padded_sequence(y,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        y, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_y, batch_first=True)  \n",
        "        masked_y = torch.masked_select(y, masks)\n",
        "        masked_out = out[masks] \n",
        "        score = F.log_softmax(masked_out, 1)\n",
        "        loss = self.loss_fn(score, masked_y)\n",
        "        _, preds  = torch.max(score, 1)\n",
        "\n",
        "\n",
        "        self.train_metrics(preds, masked_y)\n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outs):\n",
        "        m = self.train_metrics.compute()\n",
        "        self.log_dict(m, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        print('train', m)\n",
        "        self.train_metrics.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y, lengths, masks = batch\n",
        "        out = self(x, lengths)\n",
        "\n",
        "        pack_masks = torch.nn.utils.rnn.pack_padded_sequence(masks,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        masks, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_masks, batch_first=True)  \n",
        "        pack_y = torch.nn.utils.rnn.pack_padded_sequence(y,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        y, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_y, batch_first=True)  \n",
        "\n",
        "        masked_y = torch.masked_select(y, masks)\n",
        "        masked_out = out[masks] \n",
        "        score = F.log_softmax(masked_out, 1)\n",
        "        loss = self.loss_fn(score, masked_y)\n",
        "        _, preds  = torch.max(score, 1)\n",
        "\n",
        "        self.val_metrics(preds, masked_y)\n",
        "        return loss\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        # log epoch metric\n",
        "        m = self.val_metrics.compute()\n",
        "        self.log_dict(m, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        print('val', m)\n",
        "        self.val_metrics.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.015) \n",
        "        return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAKkZxXkBGxY"
      },
      "outputs": [],
      "source": [
        "debug = False\n",
        "\n",
        "train_loader, word_map, tag_map = create_conll_dataloader(train_file,\n",
        "                                                        min_word_freq=min_word_freq, \n",
        "                                                        train=True,\n",
        "                                                        debug=debug)\n",
        "val_loader, _, _ = create_conll_dataloader(val_file,\n",
        "                                            word_map=word_map, \n",
        "                                            tag_map=tag_map,\n",
        "                                            debug=debug)\n",
        "#test_loader, _, _ = create_conll_dataloader(test_file,\n",
        "#                                            min_word_freq=min_word_freq, \n",
        "#                                            debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eytoLouoUyYc"
      },
      "outputs": [],
      "source": [
        "tag_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFxuxeV9HQE8"
      },
      "outputs": [],
      "source": [
        "model = BiLSTM(len(word_map), embed_size, len(tag_map), batch_size)\n",
        "\n",
        "trainer = pl.Trainer(gpus=1,\n",
        "                     max_epochs=n_epochs, check_val_every_n_epoch=1)\n",
        "trainer.fit(model, \n",
        "            train_loader, \n",
        "            val_loader\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RITAL_NER.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "9e0ae0e97320857bbbf8ddb5dc55ef5d3da5d43c6f4a2d6fc804d21e04ae09f4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
