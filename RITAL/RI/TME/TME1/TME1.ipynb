{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from porter import stem\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"the new home has been saled on top forecasts\",\n",
    "    'the home sales rise in july',\n",
    "    'there is an increase in home sales in july',\n",
    "    'july encounter a new home sales rise'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer(s):\n",
    "    word_token = re.split(r'\\W+', s.lower())\n",
    "    word_token = [stem(w) for w in word_token if not w in stop_words]\n",
    "    return word_token\n",
    "\n",
    "def index(corpus):\n",
    "    \"\"\"\n",
    "    Return document index of corpus in param\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        idx[i] = Counter(analyzer(doc))\n",
    "    return idx\n",
    "\n",
    "def inverse_index(corpus):\n",
    "    inv_idx = {}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in analyzer(doc):\n",
    "            d = inv_idx.get(word, {})\n",
    "            nb = d.get(i, 0) + 1\n",
    "\n",
    "            d[i] = nb\n",
    "            inv_idx[word] = d\n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: Counter({'new': 1, 'home': 1, 'sale': 1, 'top': 1, 'forecast': 1}), 1: Counter({'home': 1, 'sale': 1, 'rise': 1, 'juli': 1}), 2: Counter({'increas': 1, 'home': 1, 'sale': 1, 'juli': 1}), 3: Counter({'juli': 1, 'encount': 1, 'new': 1, 'home': 1, 'sale': 1, 'rise': 1})}\n",
      "{'new': {0: 1, 3: 1}, 'home': {0: 1, 1: 1, 2: 1, 3: 1}, 'sale': {0: 1, 1: 1, 2: 1, 3: 1}, 'top': {0: 1}, 'forecast': {0: 1}, 'rise': {1: 1, 3: 1}, 'juli': {1: 1, 2: 1, 3: 1}, 'increas': {2: 1}, 'encount': {3: 1}}\n"
     ]
    }
   ],
   "source": [
    "print(index(corpus))\n",
    "print(inverse_index(corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_tfidf(corpus):\n",
    "    \"\"\"\n",
    "    Return document index of corpus in param\n",
    "    \"\"\"\n",
    "    N = len(corpus)\n",
    "    idx = index(corpus)\n",
    "    inv_idx = inverse_index(corpus)\n",
    "    vocab = inv_idx.keys()\n",
    "    idf = {word: np.log((1 + N) / (1 + len(value))) for word, value in zip(vocab, inv_idx.values())}\n",
    "\n",
    "    for doc in idx.keys():\n",
    "        for word in vocab:\n",
    "            idx[doc][word] = idx[doc][word] * idf[word]\n",
    "    return idx\n",
    "\n",
    "def inverse_index_tfidf(corpus):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: Counter({'top': 0.916290731874155, 'forecast': 0.916290731874155, 'new': 0.5108256237659907, 'home': 0.0, 'sale': 0.0, 'rise': 0.0, 'juli': 0.0, 'increas': 0.0, 'encount': 0.0}), 1: Counter({'rise': 0.5108256237659907, 'juli': 0.22314355131420974, 'home': 0.0, 'sale': 0.0, 'new': 0.0, 'top': 0.0, 'forecast': 0.0, 'increas': 0.0, 'encount': 0.0}), 2: Counter({'increas': 0.916290731874155, 'juli': 0.22314355131420974, 'home': 0.0, 'sale': 0.0, 'new': 0.0, 'top': 0.0, 'forecast': 0.0, 'rise': 0.0, 'encount': 0.0}), 3: Counter({'encount': 0.916290731874155, 'new': 0.5108256237659907, 'rise': 0.5108256237659907, 'juli': 0.22314355131420974, 'home': 0.0, 'sale': 0.0, 'top': 0.0, 'forecast': 0.0, 'increas': 0.0})}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(index_tfidf(corpus))\n",
    "print(inverse_index_tfidf(corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets, time\n",
    "import pandas as pd\n",
    "dataset = ir_datasets.load(\"beir/scifact\")\n",
    "l = []\n",
    "for doc in dataset.docs_iter():\n",
    "    l.append(doc.text)\n",
    "dataset = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f, param):\n",
    "    \"\"\"\n",
    "    Evalue les temps d'execution de la fonction f, avec le dictionnaire de paramètre reçu \n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for _ in range(5):\n",
    "        start = time.time()\n",
    "        f(**param)\n",
    "        end = time.time()\n",
    "        l.append(end - start)\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "3    2\n",
       "1    1\n",
       "2    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def question_to_token(question):\n",
    "    return analyzer(question)\n",
    "\n",
    "def TAAT(corpus, question):\n",
    "    question_word_list = question_to_token(question)\n",
    "    inv_idx = inverse_index(corpus)\n",
    "    # Filtering inv_idx to get only word that are in the question\n",
    "    inv_idx = {question_word : inv_idx[question_word] \n",
    "                for question_word in question_word_list}\n",
    "    score_dict = {}\n",
    "    for d in inv_idx.values():\n",
    "        for doc_id, score in zip(d.keys(), d.values()):\n",
    "            score_dict[doc_id] = score_dict.get(doc_id, 0) + score\n",
    "    return pd.Series(score_dict).sort_values(ascending=False)\n",
    "TAAT(corpus, 'new home')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def add_if(ds, heap, k):\n",
    "    if len(heap) k:\n",
    "        heapq.push(heap, ds)\n",
    "    elif heap[0][1] ds.score:\n",
    "        heapq.heapreplace(heap,ds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
