\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Cours}
\author{Charles Vin}
\date{Date}

\begin{document}
\maketitle

\section{Généralité}
\paragraph*{Métrique}
\begin{itemize}
    \item Taux de reconnaissance : $ \frac{bonne pred}{total pred} $ 
    \item Précision pour une classe $ c $: $ \frac{N^C_{correct}}{N^C_{predits}} $ 
    \item Rappel / Recall pour une classe $ c $ : $ \frac{N^C_{correct}}{N^C_{tot}} $
    \item F1 : need plus de vulgarisation
    \item ROC : Faux positif VS Vrais positif
    \item AUC : 
\end{itemize}

\paragraph*{Problème d'équilibre des classes}
\begin{itemize}
    \item Accuracy poubelle $\rightarrow$ Utiliser les autres metrics : AUC / ROC
    \item Ré-équilibrer le jeux de données : supprimer des données dans la classe majoritaire 
    \item Fonction de coût : pénaliser plus les erreurs dans la classe minoritaire (cours 1 diapo 51)
\end{itemize}

\paragraph*{Problème de dimension}
Ajouter un terme sur la fonction coût (ou vraisemblance) pour pénaliser le nombre (ou le poids) des coefficients utilisés pour la décision. (Cours 1 diapo 50)

\paragraph*{TF-IDF encoding} if word k is in most documents, it is probably useless $\rightarrow$ TF-Idf encoding : Donne plus de poids au keyword et un petit peu moins au stopword. A combiner avec blacklist \textbf{EXPLIQUER PLUS LES MATH ?? car ça à l'air important on 'lutilise tout le temps}

\paragraph*{Zipf law}


\section{Liste des pré-processing}
\subsection{Noisy entity removal}
\begin{itemize}
    \item Ponctuation, capital/lower case
    \item Stop word 
    \item Rare word (less than a threshold)
\end{itemize}

\subsection{Text Normalization}
\begin{itemize}
    \item Lemmatization : Lions $\rightarrow$ Lion, are $\rightarrow$ be
    \item Stemming
\end{itemize}

\subsection{Word standardisation}
Regular expression, e.g. for removing "." or expanding words’ contractions ("I’ll" → "I will")


\section{Bag of Word}

\subsection{Stengths and drawbacks}
Avantage : 
\begin{itemize}
    \item Easy light fast 
    \item Opportunity to enrich (Context encoding, Part Of Speech)
    \item Efficient implementation
    \item Still very effective with classification
\end{itemize}
Limite : 
\begin{itemize}
    \item Loose document / sentence structure : mitigated with N-gram
    \item Several task missing : POS tagging, text generation
    \item Semantic gap : On peut pas utiliser la distance euclidienne pour mesurer la différence sémantique
\end{itemize}

\subsection{Classification}
\paragraph*{Naive Bayes}
Rapide, interprétable, naturellement multiclasse. Perf à améliorer. Bien filtrer les stop word. Extention par robustesse (?)

\paragraph*{Classifieur linéaire}
Scalable, attention sensible au dimension

\section{Apprendre la sémantique}
\begin{itemize}
    \item Première approche : WordNet : trop statique (nouvelle expression, hashtag, vocab tecnique) et fait à la main 
    \item Deuxième approche : Mesure de co-occurence : "Fair \textbf{and} legitimate", "Fair \textbf{but} brutal" $\rightarrow$ Marche bien + combo avec BoW
\end{itemize}

\section{Unsupervised approches}
\subsection{LSA : Latent Semantic Analysis}
On un un vocabulaire de taille $ d $, $ N $ document, une LSA avec $ k $ greatest singular values.

On décompose la matrice $ X \in \mathbb{Q}^{N \times d}$ en \begin{itemize}
    \item $ U \in \mathbb{R}^{k \times d} $ : une ligne de $ U $ représente un vecteur de poids par mot. Les poids les plus haut donne la classe du document (?)
    \item $ \Sigma \in \mathbb{R}^{k \times k} $
    \item $ V \in \mathbb{R}^{d \times k} $ Un colonne de $ V $ représente le champs lexical d'une classe (?). C'est les mots les plus important.
    \item Est-ce que $ V $ et $ U $ sont pareil ?? 
    \item Comment on classif un nouveau document la dedans ? 
\end{itemize}

\subsubsection{Stength and drawbacks}
\begin{itemize}
    \item Bon combo avec t-SNE.
    \item Entièrement basé sur BoW : même problème  plus +\begin{itemize}
        \item Not robust to stop word (=high singular values)
        \item Problème forme négative 
    \end{itemize}
    \item Problème avec les mots rare dans les petits corpus (?)
\end{itemize}

\subsection{Kmean}
Comme d'hab, à combo avec LSA pour réduire les dimensions. 
\subsection{Dérivé LSA}
\begin{itemize}
    \item P-LSA : K-mean mais avec un assignement soft mesurer par la probabilité
    \item LDA : On rajoute un prior : modèle plus complexe $\rightarrow$ Les méthodes EM  $\rightarrow$ on utilise MCMC pour estimer les vraissemblance
    \item D'autre extension bien complexe
\end{itemize}

\end{document}