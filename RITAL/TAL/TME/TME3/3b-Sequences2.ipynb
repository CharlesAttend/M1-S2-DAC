{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8279abe5",
   "metadata": {},
   "source": [
    "# Word Embedding for Sequence Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879c909",
   "metadata": {},
   "source": [
    "**The goal of this practical is to use pre-trained word embedding for adressing the sequence prediction tasks studied in week 2: PoS and chunking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dba92",
   "metadata": {},
   "source": [
    "## 0) Loading PoS (or chunking) datasets (small or large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1478369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    listeDoc = list()\n",
    "    with open(filename, \"r\") as f:\n",
    "        doc = list()\n",
    "        for ligne in f:\n",
    "            #print \"l : \",len(ligne),\" \",ligne\n",
    "            if len(ligne) < 2: # fin de doc\n",
    "                listeDoc.append(doc)\n",
    "                doc = list()\n",
    "                continue\n",
    "            mots = ligne.replace(\"\\n\",\"\").split(\" \")\n",
    "            doc.append((mots[0],mots[2])) # mettre mots[2] Ã  la place de mots[1] pour le chuncking\n",
    "    return listeDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0514890",
   "metadata": {},
   "outputs": [],
   "source": [
    "bSmall = False\n",
    "\n",
    "if(bSmall==True):\n",
    "    filename = \"datasets/conll2000/chtrain.txt\" \n",
    "    filenameT = \"datasets/conll2000/chtest.txt\" \n",
    "\n",
    "else:\n",
    "    # Larger corpus .\n",
    "    filename = \"datasets/conll2000/train.txt\" \n",
    "    filenameT = \"datasets/conll2000/test.txt\" \n",
    "\n",
    "alldocs = load(filename)\n",
    "alldocsT = load(filenameT)\n",
    "\n",
    "print(len(alldocs),\" docs read\")\n",
    "print(len(alldocsT),\" docs (T) read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a10ae1",
   "metadata": {},
   "source": [
    "# 1) Word embedding for classifying each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fa49d",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9dc19",
   "metadata": {},
   "source": [
    "### Some token on the dataset are missing, we will encode them with a random vector\n",
    "This is sub-optimal, but we need to do something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38abc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomvec():\n",
    "    default = np.random.randn(300)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=10) # seed the randomness\n",
    "\n",
    "dictadd = dict()\n",
    "cpt=0\n",
    "for d in alldocs:\n",
    "    cpt+=1\n",
    "    print(\" ****** Document ******\",cpt)\n",
    "    for (x,pos) in d:\n",
    "        if (not (x in wv_pre_trained) and not (x in dictadd)):\n",
    "            print(x,\" not in WE, adding it with random vector\")\n",
    "            dictadd[x] = randomvec()\n",
    "            \n",
    "for d in alldocsT:\n",
    "    cpt+=1\n",
    "    print(\" ****** TEST Document ******\",cpt)\n",
    "    for (x,pos) in d:\n",
    "        if (not (x in wv_pre_trained) and not (x in dictadd)):\n",
    "            print(x,\" not in WE, adding it with random vector\")\n",
    "            dictadd[x] = randomvec()\n",
    "            #wv_pre_trained.add_vector(x,randomvec())\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94cd47",
   "metadata": {},
   "source": [
    "### Add the (key-value) 'random' word embeddings for missing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb9f7",
   "metadata": {},
   "source": [
    "### Store the train and test datasets: a word embedding for each token in the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvectors = ## YOUR CODE HERE\n",
    "wvectorsT = ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be97535",
   "metadata": {},
   "source": [
    "### Check the size of your train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ca328",
   "metadata": {},
   "source": [
    "### Collecting train/test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels train/test\n",
    "\n",
    "buf2 = [[pos for m,pos in d ] for d in alldocs]\n",
    "cles = []\n",
    "[cles.extend(b) for b in buf2]\n",
    "cles = np.unique(np.array(cles))\n",
    "cles2ind = dict(zip(cles,range(len(cles))))\n",
    "nCles = len(cles)\n",
    "print(nCles,\" keys in the dictionary\")\n",
    "\n",
    "labels  = np.array([cles2ind[pos] for d in alldocs for (m,pos) in d ])\n",
    "#np.array([cles2ind[pos] for (m,pos) in d for d in alldocs])\n",
    "labelsT  = np.array([cles2ind.setdefault(pos,len(cles)) for d in alldocsT for (m,pos) in d ])\n",
    "\n",
    "print(len(cles2ind),\" keys in the dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape)\n",
    "print(labelsT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5133930",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model! \n",
    "**An compare performances to the baseline and sequence models (HMM/CRF) or practical 2a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbd43a",
   "metadata": {},
   "source": [
    "# 2) Using word embedding with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6ce24",
   "metadata": {},
   "source": [
    "## We will define the following features functions for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3668c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_wv(sentence, index):\n",
    "    v = wv_pre_trained.get_vector(sentence[index])\n",
    "    d = {'f'+str(i):v[i] for i in range(300)}\n",
    "    return d\n",
    "\n",
    "def features_structural(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "     ## We will define the following features functions for CRF## We will define the following features functions for CRF   'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "def features_wv_plus_structural(sentence, index):\n",
    "    v = wv_pre_trained.get_vector(sentence[index]) \n",
    "    d = {'f'+str(i):v[i] for i in range(300)}\n",
    "\n",
    "    return {**d, **features_full(sentence, index)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38a9f9",
   "metadata": {},
   "source": [
    "## [Question]: explain what the 3 feature functions encode and what their differences are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6a31f",
   "metadata": {},
   "source": [
    "### You can now train a CRF with the 3 features and analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "\n",
    "tagger = ## YOUR CODE HERE\n",
    "## Train the model                  \n",
    "## Evaluate performances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
