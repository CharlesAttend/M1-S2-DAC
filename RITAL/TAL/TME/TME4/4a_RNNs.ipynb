{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlesAttend/M1-S2-DAC/blob/main/RITAL/TAL/TME/TME4/4a_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3d011d",
      "metadata": {
        "id": "4c3d011d"
      },
      "source": [
        "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
        "### (Mostly Read & Run)\n",
        "\n",
        "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "To learn to generate text, we train a recurrent neural network to do the following task:\n",
        "\n",
        "Given a \"chunk\" of text: `this is random text`\n",
        "\n",
        "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "905e7491",
      "metadata": {
        "id": "905e7491"
      },
      "source": [
        "Input ->  Output\n",
        "--------------\n",
        "T    ->    H\n",
        "H    ->    I\n",
        "I    ->    S\n",
        "S    ->    \" \"\n",
        "\" \"  ->    I\n",
        "I    ->    S\n",
        "S    ->    \" \"\n",
        "[...]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adb3f04",
      "metadata": {
        "id": "7adb3f04"
      },
      "source": [
        "\n",
        "## Load text (dataset/input.txt)\n",
        "\n",
        "Before building training batch, we load the full text in RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d03d20",
      "metadata": {
        "id": "b8d03d20"
      },
      "outputs": [],
      "source": [
        "!wget https://thome.isir.upmc.fr/classes/RITAL/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa52e79",
      "metadata": {
        "id": "1fa52e79"
      },
      "outputs": [],
      "source": [
        "! pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890c249b",
      "metadata": {
        "id": "890c249b"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(open('./input.txt').read()) #clean text => only ascii\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af54275",
      "metadata": {
        "id": "4af54275"
      },
      "source": [
        "## 2: Helper functions:\n",
        "\n",
        "We have a text and we want to feed batch of chunks to a neural network:\n",
        "\n",
        "one chunk  A,B,C,D,E\n",
        "[input] A,B,C,D -> B,C,D,E [output]\n",
        "\n",
        "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
        "\n",
        "for this, we have 3 functions:\n",
        "\n",
        "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
        "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
        "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f68d51",
      "metadata": {
        "id": "e1f68d51"
      },
      "outputs": [],
      "source": [
        "import time, math\n",
        "\n",
        "\n",
        "#Get a piece of text\n",
        "def random_chunk(chunk_len):\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(1,len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[0,c] = all_characters.index(string[c])\n",
        "    return tensor\n",
        "\n",
        "\n",
        "#Turn a piece of text in train/test\n",
        "def random_training_set(chunk_len=200, batch_size=8):\n",
        "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
        "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
        "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
        "    \n",
        "    return inp, target\n",
        "\n",
        "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141dad88",
      "metadata": {
        "id": "141dad88"
      },
      "source": [
        "## The actual RNN model (only thing to complete):\n",
        "\n",
        "It should be composed of three distinct modules:\n",
        "\n",
        "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
        "\n",
        "```\n",
        "nn.Embedding(len_dic,size_vec)\n",
        "```\n",
        "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
        "```\n",
        "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
        "```\n",
        "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
        "\n",
        "```\n",
        "nn.Linear(in_size,out_size)\n",
        "```\n",
        "=> Complete the `init` function code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d838e47",
      "metadata": {
        "id": "8d838e47"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as f\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
        "        \"\"\"\n",
        "        Create the network\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.n_char = n_char\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
        "        self.embed = ####\n",
        "        \n",
        "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
        "        self.rnn = ####\n",
        "        \n",
        "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
        "        self.predict = ####\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        batched forward: input is (batch > 1,chunk_len)\n",
        "        \"\"\"\n",
        "        input = self.embed(input)\n",
        "        output,_  = self.rnn(input)\n",
        "        output = self.predict(f.tanh(output))\n",
        "        return output\n",
        "    \n",
        "    def forward_seq(self, input,hidden=None):\n",
        "        \"\"\"\n",
        "        not batched forward: input is  (1,chunk_len)\n",
        "        \"\"\"\n",
        "        input = self.embed(input)\n",
        "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
        "        output = self.predict(f.tanh(output))\n",
        "        return output,hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34643b32",
      "metadata": {
        "collapsed": true,
        "id": "34643b32"
      },
      "source": [
        "## Text generation function\n",
        "\n",
        "Sample text from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c751d5f",
      "metadata": {
        "id": "4c751d5f"
      },
      "outputs": [],
      "source": [
        "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
        "    prime_input = char_tensor(prime_str).squeeze(0)\n",
        "    hidden = None\n",
        "    predicted = prime_str+\"\"\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "\n",
        "    for p in range(len(prime_str)-1):\n",
        "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
        "            \n",
        "    #print(hidden.size())\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
        "                # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        #print(output_dist)\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        #print(top_i)\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
        "\n",
        "    return predicted\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8783b21d",
      "metadata": {
        "collapsed": true,
        "id": "8783b21d"
      },
      "source": [
        "## Training loop for net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acbf3af",
      "metadata": {
        "id": "9acbf3af"
      },
      "outputs": [],
      "source": [
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "###Parameters\n",
        "n_epochs = 20000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 5\n",
        "lr = 0.005\n",
        "batch_size = 16\n",
        "chunk_len = 80\n",
        "\n",
        "####\n",
        "\n",
        "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
        "criterion = nn.CrossEntropyLoss() #chose criterion\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "\n",
        "def train(inp, target):\n",
        "    \"\"\"\n",
        "    Train sequence for one chunk:\n",
        "    \"\"\"\n",
        "    #reset gradients\n",
        "    model_optimizer.zero_grad() \n",
        "    \n",
        "    # predict output\n",
        "    output = model(inp)\n",
        "    \n",
        "    #compute loss\n",
        "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
        "\n",
        "    #compute gradients and backpropagate\n",
        "    loss.backward() \n",
        "    model_optimizer.step() \n",
        "\n",
        "    return loss.data.item() \n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "        print(generate(model,'Wh', 100), '\\n')\n",
        "       \n",
        "\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "392ec37e",
      "metadata": {
        "id": "392ec37e"
      },
      "source": [
        "## Visualize loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf419b5",
      "metadata": {
        "id": "caf419b5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab32c42",
      "metadata": {
        "id": "eab32c42"
      },
      "source": [
        "## Try different temperatures\n",
        "\n",
        "Changing the distribution sharpness has an impact on character sampling:\n",
        "\n",
        "more or less probable things are sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2273e75",
      "metadata": {
        "id": "b2273e75"
      },
      "outputs": [],
      "source": [
        "print(generate(model,'T', 200, temperature=1))\n",
        "print(\"----\")\n",
        "print(generate(model,'Th', 200, temperature=0.8))\n",
        "print(\"----\")\n",
        "\n",
        "print(generate(model,'Th', 200, temperature=0.5))\n",
        "print(\"----\")\n",
        "\n",
        "print(generate(model,'Th', 200, temperature=0.3))\n",
        "print(\"----\")\n",
        "\n",
        "print(generate(model,'Th', 200, temperature=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc3dea6",
      "metadata": {
        "id": "9cc3dea6"
      },
      "source": [
        "### Improving this code:\n",
        "\n",
        "(a) Tinker with parameters:\n",
        "\n",
        "- Is it really necessary to have 100 dims character embeddings\n",
        "- Chunk length can be gradually increased\n",
        "- Try changing RNN cell type (GRUs - LSTMs)\n",
        "\n",
        "(b) Add GPU support to go faster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b48332e",
      "metadata": {
        "id": "8b48332e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}